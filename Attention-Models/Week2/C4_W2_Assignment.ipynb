\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C4\_W2\_Assignment}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{assignment-2-transformer-summarizer}{%
\section{Assignment 2: Transformer
Summarizer}\label{assignment-2-transformer-summarizer}}

Welcome to the second assignment of course 4. In this assignment you
will explore summarization using the transformer model. Yes, you will
implement the transformer decoder from scratch, but we will slowly walk
you through it. There are many hints in this notebook so feel free to
use them as needed.

    \hypertarget{outline}{%
\subsection{Outline}\label{outline}}

\begin{itemize}
\tightlist
\item
  Section \ref{0}
\item
  Section \ref{1}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{11}
  \item
    Section \ref{12}
  \item
    Section \ref{13}
  \end{itemize}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{21}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex01}
    \end{itemize}
  \item
    Section \ref{22}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex02}
    \end{itemize}
  \item
    Section \ref{23}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex03}
    \end{itemize}
  \item
    Section \ref{24}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex04}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{31}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex05}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{41}
  \end{itemize}
\item
  Section \ref{5}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex06}
  \item
    Section \ref{51}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex07}
    \end{itemize}
  \end{itemize}
\end{itemize}

    \#\#\# Introduction

Summarization is an important task in natural language processing and
could be useful for a consumer enterprise. For example, bots can be used
to scrape articles, summarize them, and then you can use sentiment
analysis to identify the sentiment about certain stocks. Anyways who
wants to read an article or a long email today, when you can build a
transformer to summarize text for you. Let's get started, by completing
this assignment you will learn to:

\begin{itemize}
\tightlist
\item
  Use built-in functions to preprocess your data
\item
  Implement DotProductAttention
\item
  Implement Causal Attention
\item
  Understand how attention works
\item
  Build the transformer model
\item
  Evaluate your model
\item
  Summarize an article
\end{itemize}

As you can tell, this model is slightly different than the ones you have
already implemented. This is heavily based on attention and does not
rely on sequences, which allows for parallel computing.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{sys}
\PY{k+kn}{import} \PY{n+nn}{os}

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k+kn}{import} \PY{n+nn}{textwrap}
\PY{n}{wrapper} \PY{o}{=} \PY{n}{textwrap}\PY{o}{.}\PY{n}{TextWrapper}\PY{p}{(}\PY{n}{width}\PY{o}{=}\PY{l+m+mi}{70}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{trax}
\PY{k+kn}{from} \PY{n+nn}{trax} \PY{k+kn}{import} \PY{n}{layers} \PY{k}{as} \PY{n}{tl}
\PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{fastmath} \PY{k+kn}{import} \PY{n}{numpy} \PY{k}{as} \PY{n}{jnp}

\PY{c+c1}{\PYZsh{} to print the entire np array}
\PY{n}{np}\PY{o}{.}\PY{n}{set\PYZus{}printoptions}\PY{p}{(}\PY{n}{threshold}\PY{o}{=}\PY{n}{sys}\PY{o}{.}\PY{n}{maxsize}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
INFO:tensorflow:tokens\_length=568 inputs\_length=512 targets\_length=114
noise\_density=0.15 mean\_noise\_span\_length=3.0
    \end{Verbatim}

    \#\# Part 1: Importing the dataset

    Trax makes it easy to work with Tensorflow's datasets:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This will download the dataset if no data\PYZus{}dir is specified.}
\PY{c+c1}{\PYZsh{} Downloading and processing can take bit of time,}
\PY{c+c1}{\PYZsh{} so we have the data already in \PYZsq{}data/\PYZsq{} for you}

\PY{c+c1}{\PYZsh{} Importing CNN/DailyMail articles dataset}
\PY{n}{train\PYZus{}stream\PYZus{}fn} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{TFDS}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnn\PYZus{}dailymail}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                 \PY{n}{data\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                 \PY{n}{keys}\PY{o}{=}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{article}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{highlights}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                                 \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} This should be much faster as the data is downloaded already.}
\PY{n}{eval\PYZus{}stream\PYZus{}fn} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{TFDS}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnn\PYZus{}dailymail}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                \PY{n}{data\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                \PY{n}{keys}\PY{o}{=}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{article}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{highlights}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                                \PY{n}{train}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \#\# 1.1 Tokenize \& Detokenize helper functions

Just like in the previous assignment, the cell above loads in the
encoder for you. Given any data set, you have to be able to map words to
their indices, and indices to their words. The inputs and outputs to
your \href{https://github.com/google/trax}{Trax} models are usually
tensors of numbers where each number corresponds to a word. If you were
to process your data manually, you would have to make use of the
following:

\begin{itemize}
\tightlist
\item
  { word2Ind: } a dictionary mapping the word to its index.
\item
  { ind2Word:} a dictionary mapping the index to its word.
\item
  { word2Count:} a dictionary mapping the word to the number of times it
  appears.
\item
  { num\_words:} total number of words that have appeared.
\end{itemize}

Since you have already implemented these in previous assignments of the
specialization, we will provide you with helper functions that will do
this for you. Run the cell below to get the following functions:

\begin{itemize}
\tightlist
\item
  { tokenize: } converts a text sentence to its corresponding token list
  (i.e.~list of indices). Also converts words to subwords.
\item
  { detokenize: } converts a token list to its corresponding sentence
  (i.e.~string).
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{tokenize}\PY{p}{(}\PY{n}{input\PYZus{}str}\PY{p}{,} \PY{n}{EOS}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Input str to features dict, ready for inference\PYZdq{}\PYZdq{}\PYZdq{}}
  
    \PY{c+c1}{\PYZsh{} Use the trax.data.tokenize method. It takes streams and returns streams,}
    \PY{c+c1}{\PYZsh{} we get around it by making a 1\PYZhy{}element stream with `iter`.}
    \PY{n}{inputs} \PY{o}{=}  \PY{n+nb}{next}\PY{p}{(}\PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{tokenize}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{p}{[}\PY{n}{input\PYZus{}str}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                                      \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vocab\PYZus{}dir/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                      \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{summarize32k.subword.subwords}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Mark the end of the sentence with EOS}
    \PY{k}{return} \PY{n+nb}{list}\PY{p}{(}\PY{n}{inputs}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{n}{EOS}\PY{p}{]}

\PY{k}{def} \PY{n+nf}{detokenize}\PY{p}{(}\PY{n}{integers}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}List of ints to str\PYZdq{}\PYZdq{}\PYZdq{}}
  
    \PY{n}{s} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{detokenize}\PY{p}{(}\PY{n}{integers}\PY{p}{,}
                             \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vocab\PYZus{}dir/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                             \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{summarize32k.subword.subwords}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{wrapper}\PY{o}{.}\PY{n}{fill}\PY{p}{(}\PY{n}{s}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{preprocessing-for-language-models-concatenate-it}{%
\subsection{1.2 Preprocessing for Language Models: Concatenate
It!}\label{preprocessing-for-language-models-concatenate-it}}

This week you will use a language model -- Transformer Decoder -- to
solve an input-output problem. As you know, language models only predict
the next word, they have no notion of inputs. To create a single input
suitable for a language model, we concatenate inputs with targets
putting a separator in between. We also need to create a mask -- with 0s
at inputs and 1s at targets -- so that the model is not penalized for
mis-predicting the article and only focuses on the summary. See the
preprocess function below for how this is done.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Special tokens}
\PY{n}{SEP} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} Padding or separator token}
\PY{n}{EOS} \PY{o}{=} \PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} End of sentence token}

\PY{c+c1}{\PYZsh{} Concatenate tokenized inputs and targets using 0 as separator.}
\PY{k}{def} \PY{n+nf}{preprocess}\PY{p}{(}\PY{n}{stream}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{p}{(}\PY{n}{article}\PY{p}{,} \PY{n}{summary}\PY{p}{)} \PY{o+ow}{in} \PY{n}{stream}\PY{p}{:}
        \PY{n}{joint} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{article}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{n}{EOS}\PY{p}{,} \PY{n}{SEP}\PY{p}{]} \PY{o}{+} \PY{n+nb}{list}\PY{p}{(}\PY{n}{summary}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{n}{EOS}\PY{p}{]}\PY{p}{)}
        \PY{n}{mask} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{article}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{summary}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} Accounting for EOS and SEP}
        \PY{k}{yield} \PY{n}{joint}\PY{p}{,} \PY{n}{joint}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{mask}\PY{p}{)}

\PY{c+c1}{\PYZsh{} You can combine a few data preprocessing steps into a pipeline like this.}
\PY{n}{input\PYZus{}pipeline} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Serial}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} Tokenizes}
    \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Tokenize}\PY{p}{(}\PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vocab\PYZus{}dir/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{summarize32k.subword.subwords}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{c+c1}{\PYZsh{} Uses function defined above}
    \PY{n}{preprocess}\PY{p}{,}
    \PY{c+c1}{\PYZsh{} Filters out examples longer than 2048}
    \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{FilterByLength}\PY{p}{(}\PY{l+m+mi}{2048}\PY{p}{)}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} Apply preprocessing to data streams.}
\PY{n}{train\PYZus{}stream} \PY{o}{=} \PY{n}{input\PYZus{}pipeline}\PY{p}{(}\PY{n}{train\PYZus{}stream\PYZus{}fn}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{eval\PYZus{}stream} \PY{o}{=} \PY{n}{input\PYZus{}pipeline}\PY{p}{(}\PY{n}{eval\PYZus{}stream\PYZus{}fn}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{train\PYZus{}input}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{,} \PY{n}{train\PYZus{}mask} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{train\PYZus{}stream}\PY{p}{)}

\PY{k}{assert} \PY{n+nb}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input} \PY{o}{\PYZhy{}} \PY{n}{train\PYZus{}target}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}  \PY{c+c1}{\PYZsh{} They are the same in Language Model (LM).}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} prints mask, 0s on article, 1s on summary}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Single example mask:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{train\PYZus{}mask}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Single example mask:

 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} prints: [Example][\PYZlt{}EOS\PYZgt{}][\PYZlt{}pad\PYZgt{}][Example Summary][\PYZlt{}EOS\PYZgt{}]}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Single example:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{detokenize}\PY{p}{(}\PY{n}{train\PYZus{}input}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Single example:

 Standoff: Goerge Pickering, pictured, allegedly threatened to kill a
nurse . A distraught father who caused a four-hour standoff with
police in a Texas hospital allegedly pointed his gun at a nurse and
yelled 'I'll kill all of y'all'. Police said George Pickering, 57,
made the threats from a hospital room after becoming inconsolable over
the treatment of his son - a patient in critical care at Tomball
Regional Hospital, near Houston. Armed police and a SWAT team
descended on the medical center - and eventually convinced Pickering
to surrender after a four-hour standoff on Saturday night. Pickering
was charged with aggravated assault with a deadly weapon and is being
held on a \$30,000 bond, a statement from the Tomball Police department
said. Detectives said Pickering was in the room with his son and
family, waited for a nurse to come, then aimed his 9mm pistol at her.
He then allegedly barricaded the room and threatened to kill anybody
who came in. At the start of the confrontation, another of Pickering's
sons, who was with him in the hospital room, allegedly wrested the gun
away from him and handed it to police. Standoff: George Pickering, 57,
allegedly threatened to kill a nurse with his pistol at Tomball
Regional Hospital, sparking a police standoff . Response: Police and a
SWAT team arrived at the hospital. Pickering reportedly had one gun
taken from him, but said he had a second . Pickering then allegedly
said, 'You don't think that's the only weapon I have?', prompting
fears of a second gun and causing the lengthy showdown with police.
But when he gave himself up, police found that he was not in fact
armed. Early reports stated that Pickering had taken two hostages, but
law enforcement later said there were no captives. A spokesman for the
Tomball police department said Picerking fell ill during the standoff
and was treated in the hospital overnight - and was still there Sunday
afternoon. He does not yet have an attorney. Tense: Pickering was said
to be distraught over the condition of his son - a critical care
patient .<EOS><pad>GeorgePickering, 57, allegedly made threat from
hospital room . Police said he aimed 9mm pistol at nurse inside
Tomball Regional Hospital . Gun was allegedly wrested away from
Pickering - who said he had another . After three-hour stand-off with
police, he was found to be unarmed . Pickering has been charged with
aggravated assault with a deadly weapon .<EOS>
    \end{Verbatim}

    \hypertarget{batching-with-bucketing}{%
\subsection{1.3 Batching with bucketing}\label{batching-with-bucketing}}

As in the previous week, we use bucketing to create batches of data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Bucketing to create batched generators.}

\PY{c+c1}{\PYZsh{} Buckets are defined in terms of boundaries and batch sizes.}
\PY{c+c1}{\PYZsh{} Batch\PYZus{}sizes[i] determines the batch size for items with length \PYZlt{} boundaries[i]}
\PY{c+c1}{\PYZsh{} So below, we\PYZsq{}ll take a batch of 16 sentences of length \PYZlt{} 128 , 8 of length \PYZlt{} 256,}
\PY{c+c1}{\PYZsh{} 4 of length \PYZlt{} 512. And so on. }
\PY{n}{boundaries} \PY{o}{=}  \PY{p}{[}\PY{l+m+mi}{128}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,}  \PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{1024}\PY{p}{]}
\PY{n}{batch\PYZus{}sizes} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{16}\PY{p}{,}    \PY{l+m+mi}{8}\PY{p}{,}    \PY{l+m+mi}{4}\PY{p}{,}    \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Create the streams.}
\PY{n}{train\PYZus{}batch\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{BucketByLength}\PY{p}{(}
    \PY{n}{boundaries}\PY{p}{,} \PY{n}{batch\PYZus{}sizes}\PY{p}{)}\PY{p}{(}\PY{n}{train\PYZus{}stream}\PY{p}{)}

\PY{n}{eval\PYZus{}batch\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{BucketByLength}\PY{p}{(}
    \PY{n}{boundaries}\PY{p}{,} \PY{n}{batch\PYZus{}sizes}\PY{p}{)}\PY{p}{(}\PY{n}{eval\PYZus{}stream}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Every execution will result in generation of a different article}
\PY{c+c1}{\PYZsh{} Try running this cell multiple times to see how the length of the examples affects the batch size}
\PY{n}{input\PYZus{}batch}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{mask\PYZus{}batch} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{train\PYZus{}batch\PYZus{}stream}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Shape of the input\PYZus{}batch}
\PY{n}{input\PYZus{}batch}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(1, 1195)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print corresponding integer values}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{input\PYZus{}batch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[ 2713   132  1668    18   973    28 19300   527    28   157 11703   527
  4811    28   968 25789   157  1547   450   412    22   346   126    78
  3155  1859     3     9 19300  1353  3174   669   391   145    28  2642
  2460  2264  1248 13337  7977     6   968 13944 26939  3975 16981     2
  1120     2  1779   229 17654    16   102   144  1496   132   213 22208
   186 10255     3     9 10064     2  1779  2232   320  4756   213  1610
   132  4910  2793  1344     6  4041  1581    78  1967    78  3155  1859
     2   229   897   412    28   763  1718     2   286   320  1120    91
   292     2  1248    28  2692  1463   186    28 10722   959 13380   314
  1782   129  1435  6304   996  1019   669 27884     4   213 10064 27872
  3898 25183   318   136    10   197     3  2598   527   213  1668   676
   527  1347  8048   793 27439  9275  1628  7350  1652  4172 27439  9275
  7583     7   129    40 21090   185    64 13094     2    35    51   790
     7    26   211   116  2574   527 24692     4   101     3   129    49
     7    26  1447    64   285    22     7     5    19   132   213   201
     2    35    44    74   761     2    22     7     5   346   213   201
  2002  9175  6051     4   246  1019   846   379  8260 25067     4    11
 13337  7977 25789   157  3975 16981     8   346    12  1504   864   547
   500   824 19300     8   231    12   527   213   157  1779  1496   134
   412    22   346   213   968  2234    78  3155  1859     3    69   127
    22    40   369   553   134   171   379 16981     2   213  1859 25789
   157  1019 13337  7977     2  1353  1496   102    22 21540    21    71
    15  1081   239   128    10 21415   410    78  3155     3   514  3122
   157  9652   134   132   213  4165   621   186    41 23793    21   911
     3     9 25789   157  2232   320   179    61   809   208  1869   186
  2057    64   527   213  4165   621     3    69  9227   213 17242   587
   186 10224   320    28   260   527   986  1405     2  1779  9396    60
  2511   186   316  6105    32     3 16030     5   809  2220   186  1251
  3818  3171   132  4890   127   285 16981  1353 17654    16   186   132
  2558  1785   102  5675  1019    28  5256  1496 15811   320   213 16891
   279     3   642    15  2642  2460     2    22  4276   320 22646   186
   793   105    22    40   369   553   213   157   171     3  2713    18
    19   764   233    28 12749     4  1019   213  4811     3 15149    17
   246    11  9597   277   667 16981     2  1120     2   144 22639   651
   463   102    22  1353  1496    78  3155   379 21346     4    11     9
   157   901   320  7508   809 16981   412    22 10224   463   186 18338
  2797   246   399  3020   379  8351     4    11     9 25789   157  7712
  5256  1496 22897   320   213 22208   186 10255     2   186    28 26026
  3817  1172    15   570   379 13337  7977    43   831   285   864    25
   809   213   947    78  3112     2 14454  9043     5   527  1523  3156
   132   213  4165   621     3 16981     7     5   938     2  2907 13337
  7977 13944 26939 25702  4795  3831  4484     2    43  2441    15  4319
   884   320  2057   236   186  5180   246   399  6671    15   177  1782
 16030     5   952   320  1399   156  7270   110    22     7     5   981
  4689   213  1547  5256  1496 22897  3898   131  1113    78    68  3004
   843    78  3155   736  1782  2713  1399   156   285   122   103  1793
     7    26  1019    15 21065     5   320  5061   328    61   186  2057
   236   186  5180   246   986  1405     2   103   143    18    46    28
   769   224   488     3    69   229  3272   130  5162  2002  4795  3831
  4484   186    54 13337  7977  2125    43 15769    17  5270   157     7
     5   117    59    80 16547    75   320    31  3004  2188   132    28
  1868   527   291  1019 16981     3 13337  7977  2099   213  1614  1248
   213 19545  8500 20234  4405  8949  3324   157    80   320    50   221
   843     3 11063   251    11 16981     7     5   938 25702   127   864
  1029    22  6671    15   177   691  3156   236   186 18338  2304   246
   399   379 21346     4    11 16981   229 12370    21   346  1248    15
  2907 13337  7977  7167    78  3155    70   141   926   171    22  1353
  1496   379 16981     2  1779  2492    61   132 13499    79     2  1668
   186  2850    15 13944  2673  1054  1838  1522  3293   167     2   995
  1838   119  8602   186  1241   213   947   132   493   420     3    69
   669   391  1042  4795  3831  4484     2    28  2907 13944 26939   186
 13337  7977  3113     2   220   104     3     9   947     7     5   782
  1108     2  3306  6318   309     2   973    28  1602  2685   213  4811
    78  3155  1782  2353   824  1859 13337  7977     6 12089  1652   625
   213 17682   782   285    36   527    89   221     2  1668  2157 21679
  6475 18981  3975 16981     2    40    46  1496   132   979   527    89
   474  3898    22   127    78 13337  7977  1782    69   229   950   809
  5824  8373  2220   186  1251  3171   132  4890   102 25491  5675     3
    69   229   132  4249  1785   186 22268  1782     9  9514    81     2
   412   527   824  1087     2    23    19    46  3925     3   129   124
    19   288  1779   213  9514    81  1353   181   122    22   229  2532
   132   116   138   320 13337  7977   181    50  2125  2002  4019  3258
    11  1668   676   527  1347  8048  2021 22742  2598     2   346   186
  2024  8748 26032     5     2   616    28 13627    16    78   213  4811
    78  3155  4979     3   567  3112  1859     2    28 10064    40   234
    19    46   576    71 16994   379 17489     5    11  1396  4606  2021
   284    61   542    28  2510   756   186   615  1019   213  5256   157
    78  3155   379 19719     4    11  1668   676   527  1347  8048  2021
   273   132    28 15980    61   320    28  2510   756   192    54  2021
   615  1019   213  5256   157  5754   527  4811 16981   872   213   947
    78  3155   379 11993    11     9  5256   157  6112   412 16981  1353
 17444    71    15  1081   872   213 13337  7977 10422   132  1668   379
    69   969   285  4795  3831  4484  1353   691    68  2136     7     5
   384   132   213  2642  1782    69   229    28  3115   516   527   213
 13337  7977   228   186    89  5784   186 13904   273    64   320   134
     2 25702   186    31   228  3898    22  1113     3   252  3155  1859
     2   141   926   171    22  1353  1496     2 16981  1606   236    15
  1436  4078 15876    81  1019   213   947     7     5   117 20413   114
 17284 12323 26113    80   186  2099  1783   852   527   134  5475    28
 16379     3     9   513   229  2685  1696  1059   770   527  8134    10
     1     0 21679  6475 18981 27439  9275  1628  3975 16981     2  1779
  1353  1496   412    22   346   213   947    78  3155     2  1504   864
 19300   213  4674   669   391  1838    15  2642  2460 16346 27439  6774
  1628     9 10064   229   897   412    28   763  1718     2   286   320
  1120    91   292     2  1248    28  2692  1463   186    28 10722   959
 13380   314 16346 27439  6774  1628 16981     2  1120  4617 27439  9275
  1628  1353  1496   132   213 22208   186 10255   102  7887  9501   911
  1248   213  5256   157     2    35    22  2232   320  2057   236   186
   211   399 16346 27439  6774  1628    69   793  2021   285    22    40
   369   553   213   157   171  2104     1]
    \end{Verbatim}

    Things to notice: - First we see the corresponding values of the words.
- The first 1, which represents the
\texttt{\textless{}EOS\textgreater{}} tag of the article. - Followed by
a 0, which represents a \texttt{\textless{}pad\textgreater{}} tag. -
After the first 0 (\texttt{\textless{}pad\textgreater{}} tag) the
corresponding values are of the words that are used for the summary of
the article. - The second 1 represents the
\texttt{\textless{}EOS\textgreater{}} tag for the summary. - All the
trailing 0s represent \texttt{\textless{}pad\textgreater{}} tags which
are appended to maintain consistent length (If you don't see them then
it would mean it is already of max length)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print the article and its summary}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Article:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{detokenize}\PY{p}{(}\PY{n}{input\PYZus{}batch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Article:

 Police in Texas have released a sketch of a man suspected of shooting
a TV weatherman multiple times as he left work on Wednesday morning.
The sketch was drawn  during a hospital bed interview with KCEN-TV
meteorologist Patrick Crawford, 35, who is recovering after being shot
in the stomach and shoulder. The suspect, who managed to escape the
scene in Bruceville-Eddy on foot on Wednesday morning, is described as
a white male, 30 to 35 years old, with a medium build and a receding
hairline. 'We are actively looking for [the suspect],' Trooper D.L.
Wilson of the Texas Department of Public Safety told ABC News. 'We had
troopers out overnight, but we didn't get any calls of suspicious
people. We can't rule out that he's not in the area, but more than
likely, he's left the area.' Scroll down for video . Suspect: KCEN
weatherman Patrick Crawford (left) helped police put together this
sketch (right) of the man who shot him as he left the TV studio on
Wednesday morning. He said he had never seen him before . Crawford,
the morning weatherman for KCEN, was shot after he climbed into his
car around 9.15am on Wednesday. An unknown man approached him in the
parking lot and they exchanged words. The weatherman managed to back
up at high speed and drive out of the parking lot. He crossed the
interstate and drove to a group of construction workers, who
administered first aid and called 911. Doctors at Scott and White
Memorial Hospital in Temple said that Crawford was recovering and in
fair condition after surgery for a gunshot wound to the abdomen. From
his hospital bed, he spoke to investigators and told them he had never
seen the man before. Police have not yet found a motive for the
shooting. Gunned down: Footage shows Crawford, 35, being stretchered
away after he was shot on Wednesday . Attack: The man continued to
shoot at Crawford as he drove away and flagged down help nearby . Hit:
The weatherman sustained gunshot wounds to the stomach and shoulder,
and a bullet grazed his head . KCEN also reported that police were at
the station on Thursday, checking IDs of everyone driving in the
parking lot. Crawford's wife, fellow KCEN meteorologist Heather
Brinkmann, also revealed his quick decision to drive off and flag down
help saved his life. 'Doctors continue to tell me how well he's doing
considering the multiple gunshot wounds,' she wrote on her Facebook
page on Wednesday night. 'Police tell me that if it wasn't for his
smarts to hurry up and drive off and flag down construction workers,
it could have been a whole different story. He is truly my hero.'
Brinkmann and other KCEN employees also uploaded Superman's 'S' emblem
to their Facebook pages in a sign of support for Crawford. KCEN shared
the image with the hashtag '\#oursuperman' to its own page. Heroic:
Crawford's wife Heather said police believe he saved his life by
driving off and flagging down help . Attack: Crawford is pictured left
with his fellow KCEN hosts on Wednesday - just hours before he was
shot . Crawford, who grew up in Plano, Texas and earned his
meteorology degree from Northern Illinois University, moved from New
Orleans and joined the station in September 2012. He  married
Brinkmann, a fellow meteorologist and KCEN producer, last year. The
station's news director, Jim Hice, released a statement about the
shooting on Wednesday. 'Early this morning KCEN-HD News received the
devastating news that one of our own, Texas Today Meteorologist
Patrick Crawford, had been shot in front of our building,' he said on
KCEN. 'He is currently at Baylor Scott and White Hospital in Temple
after undergoing surgery. He is in stable condition and resting. 'The
shooter, as of this note, has not been caught. We do not know who the
shooter was or if he is connected in any way to KCEN or its
employees.' Updates: Texas Department of Public Safety officers DL
Wilson, left and Harpin Myers, give a briefing on the shooting on
Wednesday afternoon. By Thursday morning, a suspect had still not been
taken into custody . Concerns: Law enforcement officers set up near a
command post and look for the gunman on Wednesday . Response: Texas
Department of Public Safety officers go in a pickup to a command post
while other officers look for the gunman accused of shooting Crawford
outside the station on Wednesday . Rural: The gunman attacked as
Crawford was climbing into his car outside the KCEN studios in Texas .
He added that Brinkmann was by her husband's side in the hospital. 'He
is a loved member of the KCEN family and our thoughts and prayers go
out to him, Heather and their family,' he wrote. On Wednesday morning,
just hours before he was shot, Crawford showed off his winning
Christmas sweater for the station's 'Ugly Sweater Contest' and shared
images online of him raising a trophy. The town is about 75 miles
north of Austin.<EOS><pad>Meteorologist Patrick Crawford, who was shot
as he left the station on Wednesday, helped police sketch the drawing
from his hospital bed . The suspect is described as a white male, 30
to 35 years old, with a medium build and a receding hairline .
Crawford, 35, was shot in the stomach and shoulder after exchanging
words with the gunman, but he managed to drive off and get help . He
told officers that he had never seen the man before .<EOS>
    \end{Verbatim}

    You can see that the data has the following structure: - { {[}Article{]}
} -\textgreater{} \texttt{\textless{}EOS\textgreater{}} -\textgreater{}
\texttt{\textless{}pad\textgreater{}} -\textgreater{} { {[}Article
Summary{]} } -\textgreater{} \texttt{\textless{}EOS\textgreater{}}
-\textgreater{} (possibly) multiple
\texttt{\textless{}pad\textgreater{}}

The loss is taken only on the summary using cross\_entropy as loss
function.

    \# Part 2: Summarization with transformer

Now that we have given you the data generator and have handled the
preprocessing for you, it is time for you to build your own model. We
saved you some time because we know you have already preprocessed data
before in this specialization, so we would rather you spend your time
doing the next steps.

You will be implementing the attention from scratch and then using it in
your transformer model. Concretely, you will understand how attention
works, how you use it to connect the encoder and the decoder.

\#\# 2.1 Dot product attention

Now you will implement dot product attention which takes in a query,
key, value, and a mask. It returns the output.

Here are some helper functions that will help you create tensors and
display useful information: - \texttt{create\_tensor} creates a
\texttt{jax\ numpy\ array} from a list of lists. -
\texttt{display\_tensor} prints out the shape and the actual tensor.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}tensor}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Create tensor from list of lists\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{t}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{t}\PY{p}{,} \PY{n}{name}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Display shape and tensor\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{name}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{t}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{t}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Before implementing it yourself, you can play around with a toy example
of \texttt{dot\ product\ attention} without the softmax operation.
Technically it would not be \texttt{dot\ product\ attention} without the
softmax but this is done to avoid giving away too much of the answer and
the idea is to display these tensors to give you a sense of how they
look like.

The formula for attention is this one:

\[
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\
\]

\(d_{k}\) stands for the dimension of queries and keys.

The \texttt{query}, \texttt{key}, \texttt{value} and \texttt{mask}
vectors are provided for this example.

Notice that the masking is done using very negative values that will
yield a similar effect to using \$-\infty \$.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{q} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{q}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{query}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{k} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{key}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{v} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{v}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{m} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1e9}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{m}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mask}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
query shape: (2, 3)

[[1 0 0]
 [0 1 0]]

key shape: (2, 3)

[[1 2 3]
 [4 5 6]]

value shape: (2, 3)

[[0 1 0]
 [1 0 1]]

mask shape: (2, 2)

[[ 0.e+00  0.e+00]
 [-1.e+09  0.e+00]]

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{query shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{ }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{key shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[}\DecValTok{1}\AttributeTok{ }\DecValTok{2}\AttributeTok{ }\DecValTok{3}\NormalTok{]}
\AttributeTok{ }\NormalTok{[}\DecValTok{4}\AttributeTok{ }\DecValTok{5}\AttributeTok{ }\DecValTok{6}\NormalTok{]]}

\NormalTok{value shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{ }\NormalTok{[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\NormalTok{]]}

\NormalTok{mask shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{[[}\AttributeTok{ }\DecValTok{0}\ErrorTok{.e}\NormalTok{+}\BaseNTok{00}\AttributeTok{  }\DecValTok{0}\ErrorTok{.e}\NormalTok{+}\BaseNTok{00}\NormalTok{]}
\AttributeTok{ }\NormalTok{[{-}}\DecValTok{1}\ErrorTok{.e}\NormalTok{+}\ErrorTok{09}\AttributeTok{  }\DecValTok{0}\ErrorTok{.e}\NormalTok{+}\BaseNTok{00}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{q\PYZus{}dot\PYZus{}k} \PY{o}{=} \PY{n}{q} \PY{o}{@} \PY{n}{k}\PY{o}{.}\PY{n}{T} \PY{o}{/} \PY{n}{jnp}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{q\PYZus{}dot\PYZus{}k}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{query dot key}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
query dot key shape: (2, 2)

[[0.57735026 2.309401  ]
 [1.1547005  2.8867514 ]]

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{query dot key shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{[[}\DecValTok{0}\ErrorTok{.57735026}\AttributeTok{ }\DecValTok{2}\ErrorTok{.309401}\AttributeTok{  }\NormalTok{]}
\AttributeTok{ }\NormalTok{[}\DecValTok{1}\ErrorTok{.1547005}\AttributeTok{  }\DecValTok{2}\ErrorTok{.8867514}\AttributeTok{ }\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{masked} \PY{o}{=} \PY{n}{q\PYZus{}dot\PYZus{}k} \PY{o}{+} \PY{n}{m}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{masked}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{masked query dot key}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
masked query dot key shape: (2, 2)

[[ 5.7735026e-01  2.3094010e+00]
 [-1.0000000e+09  2.8867514e+00]]

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masked query dot key shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{[[}\AttributeTok{ }\DecValTok{5}\ErrorTok{.7735026e}\NormalTok{{-}}\BaseNTok{01}\AttributeTok{  }\DecValTok{2}\ErrorTok{.3094010e}\NormalTok{+}\BaseNTok{00}\NormalTok{]}
\AttributeTok{ }\NormalTok{[{-}}\DecValTok{1}\ErrorTok{.0000000e}\NormalTok{+}\ErrorTok{09}\AttributeTok{  }\DecValTok{2}\ErrorTok{.8867514e}\NormalTok{+}\BaseNTok{00}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{masked} \PY{o}{@} \PY{n}{v}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{masked query dot key dot value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
masked query dot key dot value shape: (2, 3)

[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]
 [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masked query dot key dot value shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[}\AttributeTok{ }\DecValTok{2}\ErrorTok{.3094010e}\NormalTok{+}\BaseNTok{00}\AttributeTok{  }\DecValTok{5}\ErrorTok{.7735026e}\NormalTok{{-}}\BaseNTok{01}\AttributeTok{  }\DecValTok{2}\ErrorTok{.3094010e}\NormalTok{+}\BaseNTok{00}\NormalTok{]}
\AttributeTok{ }\NormalTok{[}\AttributeTok{ }\DecValTok{2}\ErrorTok{.8867514e}\NormalTok{+}\BaseNTok{00}\AttributeTok{ }\NormalTok{{-}}\DecValTok{1}\ErrorTok{.0000000e}\NormalTok{+}\ErrorTok{09}\AttributeTok{  }\DecValTok{2}\ErrorTok{.8867514e}\NormalTok{+}\BaseNTok{00}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

    In order to use the previous dummy tensors to test some of the graded
functions, a batch dimension should be added to them so they mimic the
shape of real-life examples. The mask is also replaced by a version of
it that resembles the one that is used by trax:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{q\PYZus{}with\PYZus{}batch} \PY{o}{=} \PY{n}{q}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{q\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{query with batch dim}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{k\PYZus{}with\PYZus{}batch} \PY{o}{=} \PY{n}{k}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{k\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{key with batch dim}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{v\PYZus{}with\PYZus{}batch} \PY{o}{=} \PY{n}{v}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{v\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value with batch dim}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{m\PYZus{}bool} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{k+kc}{True}\PY{p}{,} \PY{k+kc}{True}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{k+kc}{False}\PY{p}{,} \PY{k+kc}{True}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{m\PYZus{}bool}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boolean mask}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
query with batch dim shape: (1, 2, 3)

[[[1 0 0]
  [0 1 0]]]

key with batch dim shape: (1, 2, 3)

[[[1 2 3]
  [4 5 6]]]

value with batch dim shape: (1, 2, 3)

[[[0 1 0]
  [1 0 1]]]

boolean mask shape: (2, 2)

[[ True  True]
 [False  True]]

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{query with batch dim shape: (}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]]}

\NormalTok{key with batch dim shape: (}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[[}\DecValTok{1}\AttributeTok{ }\DecValTok{2}\AttributeTok{ }\DecValTok{3}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{4}\AttributeTok{ }\DecValTok{5}\AttributeTok{ }\DecValTok{6}\NormalTok{]]]}

\NormalTok{value with batch dim shape: (}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\NormalTok{]]]}

\NormalTok{boolean mask shape: (}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{[[}\AttributeTok{ True  True}\NormalTok{]}
\AttributeTok{ }\NormalTok{[}\AttributeTok{False  True}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

    \#\#\# Exercise 01

\textbf{Instructions:} Implement the dot product attention. Concretely,
implement the following equation

\[
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\
\]

\(Q\) - query, \(K\) - key, \(V\) - values, \(M\) - mask, \({d_k}\) -
depth/dimension of the queries and keys (used for scaling down)

You can implement this formula either by \texttt{trax} numpy
(trax.math.numpy) or regular \texttt{numpy} but it is recommended to use
\texttt{jnp}.

Something to take into consideration is that within trax, the masks are
tensors of \texttt{True/False} values not 0's and \(-\infty\) as in the
previous example. Within the graded function don't think of applying the
mask by summing up matrices, instead use \texttt{jnp.where()} and treat
the \textbf{mask as a tensor of boolean values with \texttt{False} for
values that need to be masked and True for the ones that don't.}

Also take into account that the real tensors are far more complex than
the toy ones you just played with. Because of this avoid using shortened
operations such as \texttt{@} for dot product or \texttt{.T} for
transposing. Use \texttt{jnp.matmul()} and \texttt{jnp.swapaxes()}
instead.

This is the self-attention block for the transformer decoder. Good luck!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: DotProductAttention}
\PY{k}{def} \PY{n+nf}{DotProductAttention}\PY{p}{(}\PY{n}{query}\PY{p}{,} \PY{n}{key}\PY{p}{,} \PY{n}{value}\PY{p}{,} \PY{n}{mask}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Dot product self\PYZhy{}attention.}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L\PYZus{}q by d)}
\PY{l+s+sd}{        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L\PYZus{}k by d)}
\PY{l+s+sd}{        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L\PYZus{}k by d) where L\PYZus{}v = L\PYZus{}k}
\PY{l+s+sd}{        mask (jax.interpreters.xla.DeviceArray): attention\PYZhy{}mask, gates attention with shape (L\PYZus{}q by L\PYZus{}k)}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        jax.interpreters.xla.DeviceArray: Self\PYZhy{}attention array for q, k, v arrays. (L\PYZus{}q by L\PYZus{}k)}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{k}{assert} \PY{n}{query}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{n}{key}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{n}{value}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Embedding dimensions of q, k, v aren}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t all the same}\PY{l+s+s2}{\PYZdq{}}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} Save depth/dimension of the query embedding for scaling down the dot product}
    \PY{n}{depth} \PY{o}{=} \PY{n}{query}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Calculate scaled query key dot product according to formula above}
    \PY{n}{dots} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{query}\PY{p}{,} \PY{n}{jnp}\PY{o}{.}\PY{n}{swapaxes}\PY{p}{(}\PY{n}{key}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{jnp}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{depth}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Apply the mask}
    \PY{k}{if} \PY{n}{mask} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:} \PY{c+c1}{\PYZsh{} The \PYZsq{}None\PYZsq{} in this line does not need to be replaced}
        \PY{n}{dots} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{mask}\PY{p}{,} \PY{n}{dots}\PY{p}{,} \PY{n}{jnp}\PY{o}{.}\PY{n}{full\PYZus{}like}\PY{p}{(}\PY{n}{dots}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1e9}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Softmax formula implementation}
    \PY{c+c1}{\PYZsh{} Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers}
    \PY{c+c1}{\PYZsh{} Hint: Last axis should be used and keepdims should be True}
    \PY{c+c1}{\PYZsh{} Note: softmax = e\PYZca{}(dots \PYZhy{} logsumexp(dots)) = E\PYZca{}dots / sumexp(dots)}
    \PY{n}{logsumexp} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{fastmath}\PY{o}{.}\PY{n}{logsumexp}\PY{p}{(}\PY{n}{dots}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Take exponential of dots minus logsumexp to get softmax}
    \PY{c+c1}{\PYZsh{} Use jnp.exp()}
    \PY{n}{dots} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{dots} \PY{o}{\PYZhy{}} \PY{n}{logsumexp}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Multiply dots by value to get self\PYZhy{}attention}
    \PY{c+c1}{\PYZsh{} Use jnp.matmul()}
    \PY{n}{attention} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{dots}\PY{p}{,} \PY{n}{value}\PY{p}{)}

    \PY{c+c1}{\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{attention}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{DotProductAttention}\PY{p}{(}\PY{n}{q\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{n}{k\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{n}{v\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{n}{m\PYZus{}bool}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],
              [1.        , 0.        , 1.        ]]], dtype=float32)
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DeviceArray([[[}\DecValTok{0}\ErrorTok{.8496746}\AttributeTok{ }\NormalTok{,}\AttributeTok{ }\DecValTok{0}\ErrorTok{.15032545}\NormalTok{,}\AttributeTok{ }\DecValTok{0}\ErrorTok{.8496746}\AttributeTok{ }\NormalTok{],}
\AttributeTok{              }\NormalTok{[}\DecValTok{1}\ErrorTok{.}\AttributeTok{        }\NormalTok{,}\AttributeTok{ }\DecValTok{0}\ErrorTok{.}\AttributeTok{        }\NormalTok{,}\AttributeTok{ }\DecValTok{1}\ErrorTok{.}\AttributeTok{        }\NormalTok{]]], dtype=float32)}
\end{Highlighting}
\end{Shaded}

    \hypertarget{causal-attention}{%
\subsection{2.2 Causal Attention}\label{causal-attention}}

Now you are going to implement causal attention: multi-headed attention
with a mask to attend only to words that occurred before.

In the image above, a word can see everything that is before it, but not
what is after it. To implement causal attention, you will have to
transform vectors and do many reshapes. You will need to implement the
functions below.

\#\#\# Exercise 02

Implement the following functions that will be needed for Causal
Attention:

\begin{itemize}
\tightlist
\item
  { compute\_attention\_heads }: Gets an input \(x\) of dimension
  (batch\_size, seqlen, n\_heads \(\times\) d\_head) and splits the last
  (depth) dimension and stacks it to the zeroth dimension to allow
  matrix multiplication (batch\_size \(\times\) n\_heads, seqlen,
  d\_head).
\item
  { dot\_product\_self\_attention }: Creates a mask matrix with
  \texttt{False} values above the diagonal and \texttt{True} values
  below and calls DotProductAttention which implements dot product self
  attention.
\item
  { compute\_attention\_output }: Undoes compute\_attention\_heads by
  splitting first (vertical) dimension and stacking in the last (depth)
  dimension (batch\_size, seqlen, n\_heads \(\times\) d\_head). These
  operations concatenate (stack/merge) the heads.
\end{itemize}

Next there are some toy tensors which may serve to give you an idea of
the data shapes and opperations involved in Causal Attention. They are
also useful to test out your functions!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tensor2d} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{n}{q}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{tensor2d}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{query matrix (2D tensor)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{tensor4d2b} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{n}{q}\PY{p}{,} \PY{n}{q}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{n}{q}\PY{p}{,} \PY{n}{q}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{tensor4d2b}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{batch of two (multi\PYZhy{}head) collections of query matrices (4D tensor)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{tensor3dc} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{n}{jnp}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{q}\PY{p}{,} \PY{n}{q}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{tensor3dc}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{one batch of concatenated heads of query matrices (3d tensor)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{tensor3dc3b} \PY{o}{=} \PY{n}{create\PYZus{}tensor}\PY{p}{(}\PY{p}{[}\PY{n}{jnp}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{q}\PY{p}{,} \PY{n}{q}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{jnp}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{q}\PY{p}{,} \PY{n}{q}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{jnp}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{q}\PY{p}{,} \PY{n}{q}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{tensor3dc3b}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{three batches of concatenated heads of query matrices (3d tensor)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
query matrix (2D tensor) shape: (2, 3)

[[1 0 0]
 [0 1 0]]

batch of two (multi-head) collections of query matrices (4D tensor) shape: (2,
2, 2, 3)

[[[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]


 [[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]]

one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2,
6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

    \end{Verbatim}

    It is important to know that the following 3 functions would normally be
defined within the \texttt{CausalAttention} function further below.

However this makes these functions harder to test. Because of this,
these functions are shown individually using a \texttt{closure} (when
necessary) that simulates them being inside of the
\texttt{CausalAttention} function. This is done because they rely on
some variables that can be accessed from within
\texttt{CausalAttention}.

\hypertarget{support-functions}{%
\subsubsection{Support Functions}\label{support-functions}}

{ compute\_attention\_heads }: Gets an input \(x\) of dimension
(batch\_size, seqlen, n\_heads \(\times\) d\_head) and splits the last
(depth) dimension and stacks it to the zeroth dimension to allow matrix
multiplication (batch\_size \(\times\) n\_heads, seqlen, d\_head).

\textbf{For the closures you only have to fill the inner function.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}attention\PYZus{}heads\PYZus{}closure}
\PY{k}{def} \PY{n+nf}{compute\PYZus{}attention\PYZus{}heads\PYZus{}closure}\PY{p}{(}\PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Function that simulates environment inside CausalAttention function.}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        d\PYZus{}head (int):  dimensionality of heads.}
\PY{l+s+sd}{        n\PYZus{}heads (int): number of attention heads.}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        function: compute\PYZus{}attention\PYZus{}heads function}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{k}{def} \PY{n+nf}{compute\PYZus{}attention\PYZus{}heads}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Compute the attention heads.}
\PY{l+s+sd}{        Args:}
\PY{l+s+sd}{            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch\PYZus{}size, seqlen, n\PYZus{}heads X d\PYZus{}head).}
\PY{l+s+sd}{        Returns:}
\PY{l+s+sd}{            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch\PYZus{}size X n\PYZus{}heads, seqlen, d\PYZus{}head).}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
        
        \PY{c+c1}{\PYZsh{} Size of the x\PYZsq{}s batch dimension}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Length of the sequence}
        \PY{c+c1}{\PYZsh{} Should be size of x\PYZsq{}s first dimension without counting the batch dim}
        \PY{n}{seqlen} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Reshape x using jnp.reshape()}
        \PY{c+c1}{\PYZsh{} batch\PYZus{}size, seqlen, n\PYZus{}heads*d\PYZus{}head \PYZhy{}\PYZgt{} batch\PYZus{}size, seqlen, n\PYZus{}heads, d\PYZus{}head}
        \PY{n}{x} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seqlen}\PY{p}{,} \PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Transpose x using jnp.transpose()}
        \PY{c+c1}{\PYZsh{} batch\PYZus{}size, seqlen, n\PYZus{}heads, d\PYZus{}head \PYZhy{}\PYZgt{} batch\PYZus{}size, n\PYZus{}heads, seqlen, d\PYZus{}head}
        \PY{c+c1}{\PYZsh{} Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them}
        \PY{n}{x} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reshape x using jnp.reshape()}
        \PY{c+c1}{\PYZsh{} batch\PYZus{}size, n\PYZus{}heads, seqlen, d\PYZus{}head \PYZhy{}\PYZgt{} batch\PYZus{}size*n\PYZus{}heads, seqlen, d\PYZus{}head}
        \PY{n}{x} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{o}{*}\PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{seqlen}\PY{p}{,} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
        
        \PY{k}{return} \PY{n}{x}
    
    \PY{k}{return} \PY{n}{compute\PYZus{}attention\PYZus{}heads}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{tensor3dc3b}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{input tensor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{result\PYZus{}cah} \PY{o}{=} \PY{n}{compute\PYZus{}attention\PYZus{}heads\PYZus{}closure}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{(}\PY{n}{tensor3dc3b}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{result\PYZus{}cah}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{output tensor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
input tensor shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

output tensor shape: (6, 2, 3)

[[[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]]

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input tensor shape: (}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)}

\NormalTok{[[[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]]}

\NormalTok{output tensor shape: (}\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]]}
\end{Highlighting}
\end{Shaded}

    { dot\_product\_self\_attention }: Creates a mask matrix with
\texttt{False} values above the diagonal and \texttt{True} values below
and calls DotProductAttention which implements dot product self
attention.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C3}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: dot\PYZus{}product\PYZus{}self\PYZus{}attention}
\PY{k}{def} \PY{n+nf}{dot\PYZus{}product\PYZus{}self\PYZus{}attention}\PY{p}{(}\PY{n}{q}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{v}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Masked dot product self attention.}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        q (jax.interpreters.xla.DeviceArray): queries.}
\PY{l+s+sd}{        k (jax.interpreters.xla.DeviceArray): keys.}
\PY{l+s+sd}{        v (jax.interpreters.xla.DeviceArray): values.}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} Hint: mask size should be equal to L\PYZus{}q. Remember that q has shape (batch\PYZus{}size, L\PYZus{}q, d)}
    \PY{n}{mask\PYZus{}size} \PY{o}{=} \PY{n}{q}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask\PYZus{}size, mask\PYZus{}size)}
    \PY{c+c1}{\PYZsh{} Notice that 1\PYZsq{}s and 0\PYZsq{}s get casted to True/False by setting dtype to jnp.bool\PYZus{}}
    \PY{c+c1}{\PYZsh{} Use jnp.tril() \PYZhy{} Lower triangle of an array and jnp.ones()}
    \PY{n}{mask} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{tril}\PY{p}{(}\PY{n}{jnp}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{mask\PYZus{}size}\PY{p}{,} \PY{n}{mask\PYZus{}size}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{jnp}\PY{o}{.}\PY{n}{bool\PYZus{}}\PY{p}{)}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{DotProductAttention}\PY{p}{(}\PY{n}{q}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{mask}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{dot\PYZus{}product\PYZus{}self\PYZus{}attention}\PY{p}{(}\PY{n}{q\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{n}{k\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{n}{v\PYZus{}with\PYZus{}batch}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
DeviceArray([[[0.        , 1.        , 0.        ],
              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DeviceArray([[[}\DecValTok{0}\ErrorTok{.}\AttributeTok{        }\NormalTok{,}\AttributeTok{ }\DecValTok{1}\ErrorTok{.}\AttributeTok{        }\NormalTok{,}\AttributeTok{ }\DecValTok{0}\ErrorTok{.}\AttributeTok{        }\NormalTok{],}
\AttributeTok{              }\NormalTok{[}\DecValTok{0}\ErrorTok{.8496746}\AttributeTok{ }\NormalTok{,}\AttributeTok{ }\DecValTok{0}\ErrorTok{.15032543}\NormalTok{,}\AttributeTok{ }\DecValTok{0}\ErrorTok{.8496746}\AttributeTok{ }\NormalTok{]]], dtype=float32)}
\end{Highlighting}
\end{Shaded}

    { compute\_attention\_output }: Undoes compute\_attention\_heads by
splitting first (vertical) dimension and stacking in the last (depth)
dimension (batch\_size, seqlen, n\_heads \(\times\) d\_head). These
operations concatenate (stack/merge) the heads.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C4}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: compute\PYZus{}attention\PYZus{}output\PYZus{}closure}
\PY{k}{def} \PY{n+nf}{compute\PYZus{}attention\PYZus{}output\PYZus{}closure}\PY{p}{(}\PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Function that simulates environment inside CausalAttention function.}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        d\PYZus{}head (int):  dimensionality of heads.}
\PY{l+s+sd}{        n\PYZus{}heads (int): number of attention heads.}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        function: compute\PYZus{}attention\PYZus{}output function}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{k}{def} \PY{n+nf}{compute\PYZus{}attention\PYZus{}output}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Compute the attention output.}
\PY{l+s+sd}{        Args:}
\PY{l+s+sd}{            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch\PYZus{}size X n\PYZus{}heads, seqlen, d\PYZus{}head).}
\PY{l+s+sd}{        Returns:}
\PY{l+s+sd}{            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch\PYZus{}size, seqlen, n\PYZus{}heads X d\PYZus{}head).}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
        
        \PY{c+c1}{\PYZsh{} Length of the sequence}
        \PY{c+c1}{\PYZsh{} Should be size of x\PYZsq{}s first dimension without counting the batch dim}
        \PY{n}{seqlen} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Reshape x using jnp.reshape() to shape (batch\PYZus{}size, n\PYZus{}heads, seqlen, d\PYZus{}head)}
        \PY{n}{x} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{seqlen}\PY{p}{,} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Transpose x using jnp.transpose() to shape (batch\PYZus{}size, seqlen, n\PYZus{}heads, d\PYZus{}head)}
        \PY{n}{x} \PY{o}{=} \PY{n}{jnp}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1} \PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
        
        \PY{c+c1}{\PYZsh{} Reshape to allow to concatenate the heads}
        \PY{k}{return} \PY{n}{jnp}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{seqlen}\PY{p}{,} \PY{n}{n\PYZus{}heads} \PY{o}{*} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{compute\PYZus{}attention\PYZus{}output}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{result\PYZus{}cah}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{input tensor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{result\PYZus{}cao} \PY{o}{=} \PY{n}{compute\PYZus{}attention\PYZus{}output\PYZus{}closure}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{(}\PY{n}{result\PYZus{}cah}\PY{p}{)}
\PY{n}{display\PYZus{}tensor}\PY{p}{(}\PY{n}{result\PYZus{}cao}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{output tensor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
input tensor shape: (6, 2, 3)

[[[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]]

output tensor shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input tensor shape: (}\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\NormalTok{[[[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]]}

\NormalTok{output tensor shape: (}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)}

\NormalTok{[[[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]}

\NormalTok{ [[}\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\NormalTok{]}
\AttributeTok{  }\NormalTok{[}\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{0}\AttributeTok{ }\DecValTok{1}\AttributeTok{ }\DecValTok{0}\NormalTok{]]]}
\end{Highlighting}
\end{Shaded}

    \hypertarget{causal-attention-function}{%
\subsubsection{Causal Attention
Function}\label{causal-attention-function}}

Now it is time for you to put everything together within the
\texttt{CausalAttention} or Masked multi-head attention function:

    \textbf{Instructions:} Implement the causal attention. Your model
returns the causal attention through a \(tl.Serial\) with the following:

\begin{itemize}
\tightlist
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Branch}{tl.Branch}
  }: consisting of 3 {[}tl.Dense(d\_feature), ComputeAttentionHeads{]}
  to account for the queries, keys, and values.
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.base.Fn}{tl.Fn}}:
  Takes in dot\_product\_self\_attention function and uses it to compute
  the dot product using \(Q\), \(K\), \(V\).
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.base.Fn}{tl.Fn}}:
  Takes in compute\_attention\_output\_closure to allow for parallel
  computing.
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dense}{tl.Dense}}:
  Final Dense layer, with dimension \texttt{d\_feature}.
\end{itemize}

Remember that in order for trax to properly handle the functions you
just defined, they need to be added as layers using the
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.base.Fn}{\texttt{tl.Fn()}}
function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C5}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: CausalAttention}
\PY{k}{def} \PY{n+nf}{CausalAttention}\PY{p}{(}\PY{n}{d\PYZus{}feature}\PY{p}{,} 
                    \PY{n}{n\PYZus{}heads}\PY{p}{,} 
                    \PY{n}{compute\PYZus{}attention\PYZus{}heads\PYZus{}closure}\PY{o}{=}\PY{n}{compute\PYZus{}attention\PYZus{}heads\PYZus{}closure}\PY{p}{,}
                    \PY{n}{dot\PYZus{}product\PYZus{}self\PYZus{}attention}\PY{o}{=}\PY{n}{dot\PYZus{}product\PYZus{}self\PYZus{}attention}\PY{p}{,}
                    \PY{n}{compute\PYZus{}attention\PYZus{}output\PYZus{}closure}\PY{o}{=}\PY{n}{compute\PYZus{}attention\PYZus{}output\PYZus{}closure}\PY{p}{,}
                    \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Transformer\PYZhy{}style multi\PYZhy{}headed causal attention.}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        d\PYZus{}feature (int):  dimensionality of feature embedding.}
\PY{l+s+sd}{        n\PYZus{}heads (int): number of attention heads.}
\PY{l+s+sd}{        compute\PYZus{}attention\PYZus{}heads\PYZus{}closure (function): Closure around compute\PYZus{}attention heads.}
\PY{l+s+sd}{        dot\PYZus{}product\PYZus{}self\PYZus{}attention (function): dot\PYZus{}product\PYZus{}self\PYZus{}attention function. }
\PY{l+s+sd}{        compute\PYZus{}attention\PYZus{}output\PYZus{}closure (function): Closure around compute\PYZus{}attention\PYZus{}output. }
\PY{l+s+sd}{        mode (str): \PYZsq{}train\PYZsq{} or \PYZsq{}eval\PYZsq{}.}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        trax.layers.combinators.Serial: Multi\PYZhy{}headed self\PYZhy{}attention model.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{k}{assert} \PY{n}{d\PYZus{}feature} \PY{o}{\PYZpc{}} \PY{n}{n\PYZus{}heads} \PY{o}{==} \PY{l+m+mi}{0}
    \PY{n}{d\PYZus{}head} \PY{o}{=} \PY{n}{d\PYZus{}feature} \PY{o}{/}\PY{o}{/} \PY{n}{n\PYZus{}heads}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)}
    \PY{c+c1}{\PYZsh{} Since you are dealing with closures you might need to call the outer }
    \PY{c+c1}{\PYZsh{} function with the correct parameters to get the actual uncalled function.}
    \PY{n}{ComputeAttentionHeads} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Fn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AttnHeads}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{compute\PYZus{}attention\PYZus{}heads\PYZus{}closure}\PY{p}{(}\PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}out}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        

    \PY{k}{return} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Branch}\PY{p}{(} \PY{c+c1}{\PYZsh{} creates three towers for one input, takes activations and creates queries keys and values}
            \PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{d\PYZus{}feature}\PY{p}{)}\PY{p}{,} \PY{n}{ComputeAttentionHeads}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} queries}
            \PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{d\PYZus{}feature}\PY{p}{)}\PY{p}{,} \PY{n}{ComputeAttentionHeads}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} keys}
            \PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{d\PYZus{}feature}\PY{p}{)}\PY{p}{,} \PY{n}{ComputeAttentionHeads}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} values}
        \PY{p}{)}\PY{p}{,}
        
        \PY{n}{tl}\PY{o}{.}\PY{n}{Fn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DotProductAttn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dot\PYZus{}product\PYZus{}self\PYZus{}attention}\PY{p}{,} \PY{n}{n\PYZus{}out}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} takes QKV}
        \PY{c+c1}{\PYZsh{} HINT: The second argument to tl.Fn() is an uncalled function}
        \PY{c+c1}{\PYZsh{} Since you are dealing with closures you might need to call the outer }
        \PY{c+c1}{\PYZsh{} function with the correct parameters to get the actual uncalled function.}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Fn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AttnOutput}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{compute\PYZus{}attention\PYZus{}output\PYZus{}closure}\PY{p}{(}\PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{d\PYZus{}head}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}out}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} to allow for parallel}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{d\PYZus{}feature}\PY{p}{)} \PY{c+c1}{\PYZsh{} Final dense layer}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Take a look at the causal attention model}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{CausalAttention}\PY{p}{(}\PY{n}{d\PYZus{}feature}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{n\PYZus{}heads}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Serial[
  Branch\_out3[
    [Dense\_512, AttnHeads]
    [Dense\_512, AttnHeads]
    [Dense\_512, AttnHeads]
  ]
  DotProductAttn\_in3
  AttnOutput
  Dense\_512
]
    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Serial[}
\NormalTok{  Branch\_out3[}
\NormalTok{    [Dense\_512, AttnHeads]}
\NormalTok{    [Dense\_512, AttnHeads]}
\NormalTok{    [Dense\_512, AttnHeads]}
\NormalTok{  ]}
\NormalTok{  DotProductAttn\_in3}
\NormalTok{  AttnOutput}
\NormalTok{  Dense\_512}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

    \hypertarget{transformer-decoder-block}{%
\subsection{2.3 Transformer decoder
block}\label{transformer-decoder-block}}

Now that you have implemented the causal part of the transformer, you
will implement the transformer decoder block. Concretely you will be
implementing this image now.

To implement this function, you will have to call the
\texttt{CausalAttention} or Masked multi-head attention function you
implemented above. You will have to add a feedforward which consists of:

\begin{itemize}
\tightlist
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.normalization.LayerNorm}{tl.LayerNorm}
  }: used to layer normalize
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dense}{tl.Dense}
  }: the dense layer
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.activation_fns.Relu}{ff\_activation}
  }: feed forward activation (we use ReLu) here.
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dropout}{tl.Dropout}
  }: dropout layer
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dense}{tl.Dense}
  }: dense layer
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dropout}{tl.Dropout}
  }: dropout layer
\end{itemize}

Finally once you implement the feedforward, you can go ahead and
implement the entire block using:

\begin{itemize}
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Residual}{tl.Residual}
  }: takes in the tl.LayerNorm(), causal attention block, tl.dropout.
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Residual}{tl.Residual}
  }: takes in the feedforward block you will implement.
\end{itemize}

\#\#\# Exercise 03 \textbf{Instructions:} Implement the transformer
decoder block. Good luck!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C6}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: DecoderBlock}
\PY{k}{def} \PY{n+nf}{DecoderBlock}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{d\PYZus{}ff}\PY{p}{,} \PY{n}{n\PYZus{}heads}\PY{p}{,}
                 \PY{n}{dropout}\PY{p}{,} \PY{n}{mode}\PY{p}{,} \PY{n}{ff\PYZus{}activation}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns a list of layers that implements a Transformer decoder block.}

\PY{l+s+sd}{    The input is an activation tensor.}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        d\PYZus{}model (int):  depth of embedding.}
\PY{l+s+sd}{        d\PYZus{}ff (int): depth of feed\PYZhy{}forward layer.}
\PY{l+s+sd}{        n\PYZus{}heads (int): number of attention heads.}
\PY{l+s+sd}{        dropout (float): dropout rate (how much to drop out).}
\PY{l+s+sd}{        mode (str): \PYZsq{}train\PYZsq{} or \PYZsq{}eval\PYZsq{}.}
\PY{l+s+sd}{        ff\PYZus{}activation (function): the non\PYZhy{}linearity in feed\PYZhy{}forward layer.}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} Create masked multi\PYZhy{}head attention block using CausalAttention function}
    \PY{n}{causal\PYZus{}attention} \PY{o}{=} \PY{n}{CausalAttention}\PY{p}{(} 
                        \PY{n}{d\PYZus{}model}\PY{p}{,}
                        \PY{n}{n\PYZus{}heads}\PY{o}{=}\PY{n}{n\PYZus{}heads}\PY{p}{,}
                        \PY{n}{mode}\PY{o}{=}\PY{n}{mode}
                        \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Create feed\PYZhy{}forward block (list) with two dense layers with dropout and input normalized}
    \PY{n}{feed\PYZus{}forward} \PY{o}{=} \PY{p}{[} 
        \PY{c+c1}{\PYZsh{} Normalize layer inputs}
        \PY{n}{tl}\PY{o}{.}\PY{n}{LayerNorm}\PY{p}{(}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Add first feed forward (dense) layer (don\PYZsq{}t forget to set the correct value for n\PYZus{}units)}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{d\PYZus{}ff}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Add activation function passed in as a parameter (you need to call it!)}
        \PY{n}{ff\PYZus{}activation}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Generally ReLU}
        \PY{c+c1}{\PYZsh{} Add dropout with rate and mode specified (i.e., don\PYZsq{}t use dropout during evaluation)}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{rate}\PY{o}{=}\PY{n}{dropout}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Add second feed forward layer (don\PYZsq{}t forget to set the correct value for n\PYZus{}units)}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Add dropout with rate and mode specified (i.e., don\PYZsq{}t use dropout during evaluation)}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{rate}\PY{o}{=}\PY{n}{dropout}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}
    \PY{p}{]}

    \PY{c+c1}{\PYZsh{} Add list of two Residual blocks: the attention with normalization and dropout and feed\PYZhy{}forward blocks}
    \PY{k}{return} \PY{p}{[}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Residual}\PY{p}{(}
          \PY{c+c1}{\PYZsh{} Normalize layer input}
          \PY{n}{tl}\PY{o}{.}\PY{n}{LayerNorm}\PY{p}{(}\PY{p}{)}\PY{p}{,}
          \PY{c+c1}{\PYZsh{} Add causal attention block previously defined (without parentheses)}
          \PY{n}{causal\PYZus{}attention}\PY{p}{,}
          \PY{c+c1}{\PYZsh{} Add dropout with rate and mode specified}
          \PY{n}{tl}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{rate}\PY{o}{=}\PY{n}{dropout}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}
        \PY{p}{)}\PY{p}{,}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Residual}\PY{p}{(}
          \PY{c+c1}{\PYZsh{} Add feed forward block (without parentheses)}
          \PY{n}{feed\PYZus{}forward}
        \PY{p}{)}\PY{p}{,}
      \PY{p}{]}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Take a look at the decoder block}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{DecoderBlock}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{d\PYZus{}ff}\PY{o}{=}\PY{l+m+mi}{2048}\PY{p}{,} \PY{n}{n\PYZus{}heads}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,} \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ff\PYZus{}activation}\PY{o}{=}\PY{n}{tl}\PY{o}{.}\PY{n}{Relu}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[Serial[
  Branch\_out2[
    None
    Serial[
      LayerNorm
      Serial[
        Branch\_out3[
          [Dense\_512, AttnHeads]
          [Dense\_512, AttnHeads]
          [Dense\_512, AttnHeads]
        ]
        DotProductAttn\_in3
        AttnOutput
        Dense\_512
      ]
      Dropout
    ]
  ]
  Add\_in2
], Serial[
  Branch\_out2[
    None
    Serial[
      LayerNorm
      Dense\_2048
      Relu
      Dropout
      Dense\_512
      Dropout
    ]
  ]
  Add\_in2
]]
    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[Serial[}
\NormalTok{  Branch\_out2[}
\NormalTok{    None}
\NormalTok{    Serial[}
\NormalTok{      LayerNorm}
\NormalTok{      Serial[}
\NormalTok{        Branch\_out3[}
\NormalTok{          [Dense\_512, AttnHeads]}
\NormalTok{          [Dense\_512, AttnHeads]}
\NormalTok{          [Dense\_512, AttnHeads]}
\NormalTok{        ]}
\NormalTok{        DotProductAttn\_in3}
\NormalTok{        AttnOutput}
\NormalTok{        Dense\_512}
\NormalTok{      ]}
\NormalTok{      Dropout}
\NormalTok{    ]}
\NormalTok{  ]}
\NormalTok{  Add\_in2}
\NormalTok{], Serial[}
\NormalTok{  Branch\_out2[}
\NormalTok{    None}
\NormalTok{    Serial[}
\NormalTok{      LayerNorm}
\NormalTok{      Dense\_2048}
\NormalTok{      Relu}
\NormalTok{      Dropout}
\NormalTok{      Dense\_512}
\NormalTok{      Dropout}
\NormalTok{    ]}
\NormalTok{  ]}
\NormalTok{  Add\_in2}
\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

    \#\# 2.4 Transformer Language Model

You will now bring it all together. In this part you will use all the
subcomponents you previously built to make the final model. Concretely,
here is the image you will be implementing.

\#\#\# Exercise 04 \textbf{Instructions:} Previously you coded the
decoder block. Now you will code the transformer language model. Here is
what you will need.

\begin{itemize}
\tightlist
\item
  { positional\_enconder }- a list containing the following layers:

  \begin{itemize}
  \tightlist
  \item

    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Embedding}{tl.Embedding}
  \item

    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dropout}{tl.Dropout}
  \item

    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.attention.PositionalEncoding}{tl.PositionalEncoding}
  \end{itemize}
\item
  A list of \texttt{n\_layers} { decoder blocks}.
\item
  {
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Serial}{tl.Serial}:
  } takes in the following layers or lists of layers:

  \begin{itemize}
  \tightlist
  \item
    {
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.attention.ShiftRight}{tl.ShiftRight}:
    }: shift the tensor to the right by padding on axis 1.
  \item
    { positional\_encoder }: encodes the text positions.
  \item
    { decoder\_blocks }: the ones you created.
  \item
    {
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.normalization.LayerNorm}{tl.LayerNorm}
    }: a layer norm.
  \item
    {
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dense}{tl.Dense}
    }: takes in the vocab\_size.
  \item
    {
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.LogSoftmax}{tl.LogSoftmax}
    }: to predict.
  \end{itemize}
\end{itemize}

Go go go!! You can do it :)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C7}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: TransformerLM}
\PY{k}{def} \PY{n+nf}{TransformerLM}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{33300}\PY{p}{,}
                  \PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,}
                  \PY{n}{d\PYZus{}ff}\PY{o}{=}\PY{l+m+mi}{2048}\PY{p}{,}
                  \PY{n}{n\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{,}
                  \PY{n}{n\PYZus{}heads}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,}
                  \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                  \PY{n}{max\PYZus{}len}\PY{o}{=}\PY{l+m+mi}{4096}\PY{p}{,}
                  \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{n}{ff\PYZus{}activation}\PY{o}{=}\PY{n}{tl}\PY{o}{.}\PY{n}{Relu}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns a Transformer language model.}

\PY{l+s+sd}{    The input to the model is a tensor of tokens. (This model uses only the}
\PY{l+s+sd}{    decoder part of the overall Transformer.)}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        vocab\PYZus{}size (int): vocab size.}
\PY{l+s+sd}{        d\PYZus{}model (int):  depth of embedding.}
\PY{l+s+sd}{        d\PYZus{}ff (int): depth of feed\PYZhy{}forward layer.}
\PY{l+s+sd}{        n\PYZus{}layers (int): number of decoder layers.}
\PY{l+s+sd}{        n\PYZus{}heads (int): number of attention heads.}
\PY{l+s+sd}{        dropout (float): dropout rate (how much to drop out).}
\PY{l+s+sd}{        max\PYZus{}len (int): maximum symbol length for positional encoding.}
\PY{l+s+sd}{        mode (str): \PYZsq{}train\PYZsq{}, \PYZsq{}eval\PYZsq{} or \PYZsq{}predict\PYZsq{}, predict mode is for fast inference.}
\PY{l+s+sd}{        ff\PYZus{}activation (function): the non\PYZhy{}linearity in feed\PYZhy{}forward layer.}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens}
\PY{l+s+sd}{        to activations over a vocab set.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} Embedding inputs and positional encoder}
    \PY{n}{positional\PYZus{}encoder} \PY{o}{=} \PY{p}{[} 
        \PY{c+c1}{\PYZsh{} Add embedding layer of dimension (vocab\PYZus{}size, d\PYZus{}model)}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Use dropout with rate and mode specified}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{rate}\PY{o}{=}\PY{n}{dropout}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Add positional encoding layer with maximum input length and mode specified}
        \PY{n}{tl}\PY{o}{.}\PY{n}{PositionalEncoding}\PY{p}{(}\PY{n}{max\PYZus{}len}\PY{o}{=}\PY{n}{max\PYZus{}len}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}
    \PY{p}{]}

    \PY{c+c1}{\PYZsh{} Create stack (list) of decoder blocks with n\PYZus{}layers with necessary parameters}
    \PY{n}{decoder\PYZus{}blocks} \PY{o}{=} \PY{p}{[} 
        \PY{n}{DecoderBlock}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{d\PYZus{}ff}\PY{p}{,} \PY{n}{n\PYZus{}heads}\PY{p}{,} \PY{n}{dropout}\PY{p}{,} \PY{n}{mode}\PY{p}{,} \PY{n}{ff\PYZus{}activation}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}layers}\PY{p}{)}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Create the complete model as written in the figure}
    \PY{k}{return} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(}
        \PY{c+c1}{\PYZsh{} Use teacher forcing (feed output of previous step to current step)}
        \PY{n}{tl}\PY{o}{.}\PY{n}{ShiftRight}\PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Specify the mode!}
        \PY{c+c1}{\PYZsh{} Add positional encoder}
        \PY{n}{positional\PYZus{}encoder}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Add decoder blocks}
        \PY{n}{decoder\PYZus{}blocks}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Normalize layer}
        \PY{n}{tl}\PY{o}{.}\PY{n}{LayerNorm}\PY{p}{(}\PY{p}{)}\PY{p}{,}

        \PY{c+c1}{\PYZsh{} Add dense layer of vocab\PYZus{}size (since need to select a word to translate to)}
        \PY{c+c1}{\PYZsh{} (a.k.a., logits layer. Note: activation already set by ff\PYZus{}activation)}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{p}{)}\PY{p}{,}
        \PY{c+c1}{\PYZsh{} Get probabilities with Logsoftmax}
        \PY{n}{tl}\PY{o}{.}\PY{n}{LogSoftmax}\PY{p}{(}\PY{p}{)}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Take a look at the Transformer}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{TransformerLM}\PY{p}{(}\PY{n}{n\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Serial[
  ShiftRight(1)
  Embedding\_33300\_512
  Dropout
  PositionalEncoding
  Serial[
    Branch\_out2[
      None
      Serial[
        LayerNorm
        Serial[
          Branch\_out3[
            [Dense\_512, AttnHeads]
            [Dense\_512, AttnHeads]
            [Dense\_512, AttnHeads]
          ]
          DotProductAttn\_in3
          AttnOutput
          Dense\_512
        ]
        Dropout
      ]
    ]
    Add\_in2
  ]
  Serial[
    Branch\_out2[
      None
      Serial[
        LayerNorm
        Dense\_2048
        Relu
        Dropout
        Dense\_512
        Dropout
      ]
    ]
    Add\_in2
  ]
  LayerNorm
  Dense\_33300
  LogSoftmax
]
    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Serial[}
\NormalTok{  ShiftRight(}\DecValTok{1}\NormalTok{)}
\NormalTok{  Embedding\_33300\_512}
\NormalTok{  Dropout}
\NormalTok{  PositionalEncoding}
\NormalTok{  Serial[}
\NormalTok{    Branch\_out2[}
\NormalTok{      None}
\NormalTok{      Serial[}
\NormalTok{        LayerNorm}
\NormalTok{        Serial[}
\NormalTok{          Branch\_out3[}
\NormalTok{            [Dense\_512, AttnHeads]}
\NormalTok{            [Dense\_512, AttnHeads]}
\NormalTok{            [Dense\_512, AttnHeads]}
\NormalTok{          ]}
\NormalTok{          DotProductAttn\_in3}
\NormalTok{          AttnOutput}
\NormalTok{          Dense\_512}
\NormalTok{        ]}
\NormalTok{        Dropout}
\NormalTok{      ]}
\NormalTok{    ]}
\NormalTok{    Add\_in2}
\NormalTok{  ]}
\NormalTok{  Serial[}
\NormalTok{    Branch\_out2[}
\NormalTok{      None}
\NormalTok{      Serial[}
\NormalTok{        LayerNorm}
\NormalTok{        Dense\_2048}
\NormalTok{        Relu}
\NormalTok{        Dropout}
\NormalTok{        Dense\_512}
\NormalTok{        Dropout}
\NormalTok{      ]}
\NormalTok{    ]}
\NormalTok{    Add\_in2}
\NormalTok{  ]}
\NormalTok{  LayerNorm}
\NormalTok{  Dense\_33300}
\NormalTok{  LogSoftmax}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

    \# Part 3: Training

Now you are going to train your model. As usual, you have to define the
cost function, the optimizer, and decide whether you will be training it
on a \texttt{gpu} or \texttt{cpu}. In this case, you will train your
model on a cpu for a few steps and we will load in a pre-trained model
that you can use to predict with your own words.

    \#\#\# 3.1 Training the model

You will now write a function that takes in your model and trains it. To
train your model you have to decide how many times you want to iterate
over the entire data set. Each iteration is defined as an
\texttt{epoch}. For each epoch, you have to go over all the data, using
your training iterator.

\#\#\# Exercise 05 \textbf{Instructions:} Implement the
\texttt{train\_model} program below to train the neural network above.
Here is a list of things you should do:

\begin{itemize}
\tightlist
\item
  Create the train task by calling
  \href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.training.TrainTask}{\texttt{trax.supervised.training.TrainTask}}
  and pass in the following:

  \begin{itemize}
  \tightlist
  \item
    { labeled\_data } = train\_gen
  \item
    { loss\_fn } =
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.metrics.CrossEntropyLoss}{tl.CrossEntropyLoss()}
  \item
    { optimizer } =
    \href{https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html\#trax.optimizers.adam.Adam}{trax.optimizers.Adam(0.01)}
  \item
    { lr\_schedule } =
    \href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.lr_schedules.warmup_and_rsqrt_decay}{lr\_schedule}
  \end{itemize}
\item
  Create the eval task by calling
  \href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.training.EvalTask}{\texttt{trax.supervised.training.EvalTask}}
  and pass in the following:

  \begin{itemize}
  \tightlist
  \item
    { labeled\_data } = eval\_gen
  \item
    { metrics } = tl.CrossEntropyLoss() and
    \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.metrics.Accuracy}{tl.Accuracy()}
  \end{itemize}
\item
  Create the training loop by calling
  \href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.training.Loop}{\texttt{trax.supervised.Training.Loop}}
  and pass in the following:

  \begin{itemize}
  \tightlist
  \item
    { TransformerLM }
  \item
    { train\_task }
  \item
    { eval\_task } = {[}eval\_task{]}
  \item
    { output\_dir} = output\_dir
  \end{itemize}
\end{itemize}

You will be using a cross entropy loss, with Adam optimizer. Please read
the \href{https://trax-ml.readthedocs.io/en/latest/index.html}{Trax}
documentation to get a full understanding.

The training loop that this function returns can be runned using the
\texttt{run()} method by passing in the desired number of steps.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{supervised} \PY{k+kn}{import} \PY{n}{training}

\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C8}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: train\PYZus{}model}
\PY{k}{def} \PY{n+nf}{training\PYZus{}loop}\PY{p}{(}\PY{n}{TransformerLM}\PY{p}{,} \PY{n}{train\PYZus{}gen}\PY{p}{,} \PY{n}{eval\PYZus{}gen}\PY{p}{,} \PY{n}{output\PYZus{}dir} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZti{}/model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{    Input:}
\PY{l+s+sd}{        TransformerLM (trax.layers.combinators.Serial): The model you are building.}
\PY{l+s+sd}{        train\PYZus{}gen (generator): Training stream of data.}
\PY{l+s+sd}{        eval\PYZus{}gen (generator): Evaluation stream of data.}
\PY{l+s+sd}{        output\PYZus{}dir (str): folder to save your file.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        trax.supervised.training.Loop: Training loop.}
\PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
    \PY{n}{output\PYZus{}dir} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{expanduser}\PY{p}{(}\PY{n}{output\PYZus{}dir}\PY{p}{)}  \PY{c+c1}{\PYZsh{} trainer is an object}
    \PY{n}{lr\PYZus{}schedule} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{lr}\PY{o}{.}\PY{n}{warmup\PYZus{}and\PYZus{}rsqrt\PYZus{}decay}\PY{p}{(}\PY{n}{n\PYZus{}warmup\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{max\PYZus{}value}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{n}{train\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{TrainTask}\PY{p}{(} 
      \PY{n}{labeled\PYZus{}data}\PY{o}{=}\PY{n}{train\PYZus{}gen}\PY{p}{,} \PY{c+c1}{\PYZsh{} The training generator}
      \PY{n}{loss\PYZus{}layer}\PY{o}{=}\PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Loss function }
      \PY{n}{optimizer}\PY{o}{=}\PY{n}{trax}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Optimizer (Don\PYZsq{}t forget to set LR to 0.01)}
      \PY{n}{lr\PYZus{}schedule}\PY{o}{=}\PY{n}{lr\PYZus{}schedule}\PY{p}{,}
      \PY{n}{n\PYZus{}steps\PYZus{}per\PYZus{}checkpoint}\PY{o}{=}\PY{l+m+mi}{10}
    \PY{p}{)}

    \PY{n}{eval\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{EvalTask}\PY{p}{(} 
      \PY{n}{labeled\PYZus{}data}\PY{o}{=}\PY{n}{eval\PYZus{}gen}\PY{p}{,} \PY{c+c1}{\PYZsh{} The evaluation generator}
      \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{tl}\PY{o}{.}\PY{n}{Accuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]} \PY{c+c1}{\PYZsh{} CrossEntropyLoss and Accuracy}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}

    \PY{n}{loop} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{Loop}\PY{p}{(}\PY{n}{TransformerLM}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,}
                                       \PY{n}{d\PYZus{}ff}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,}
                                       \PY{n}{n\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
                                       \PY{n}{n\PYZus{}heads}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
                                       \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                         \PY{n}{train\PYZus{}task}\PY{p}{,}
                         \PY{n}{eval\PYZus{}tasks}\PY{o}{=}\PY{p}{[}\PY{n}{eval\PYZus{}task}\PY{p}{]}\PY{p}{,}
                         \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{n}{output\PYZus{}dir}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{loop}
\end{Verbatim}
\end{tcolorbox}

    Notice that the model will be trained for only 10 steps.

Even with this constraint the model with the original default arguments
took a very long time to finish. Because of this some parameters are
changed when defining the model that is fed into the training loop in
the function above.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Should take around 1.5 minutes}
\PY{o}{!}rm \PYZhy{}f \PYZti{}/model/model.pkl.gz
\PY{n}{loop} \PY{o}{=} \PY{n}{training\PYZus{}loop}\PY{p}{(}\PY{n}{TransformerLM}\PY{p}{,} \PY{n}{train\PYZus{}batch\PYZus{}stream}\PY{p}{,} \PY{n}{eval\PYZus{}batch\PYZus{}stream}\PY{p}{)}
\PY{n}{loop}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Step      1: Ran 1 train steps in 8.22 secs
Step      1: train CrossEntropyLoss |  10.41255665
Step      1: eval  CrossEntropyLoss |  10.41328621
Step      1: eval          Accuracy |  0.00000000

Step     10: Ran 9 train steps in 65.82 secs
Step     10: train CrossEntropyLoss |  10.41382504
Step     10: eval  CrossEntropyLoss |  10.41363621
Step     10: eval          Accuracy |  0.00000000
    \end{Verbatim}

    \# Part 4: Evaluation

\#\#\# 4.1 Loading in a trained model

In this part you will evaluate by loading in an almost exact version of
the model you coded, but we trained it for you to save you time. Please
run the cell below to load in the model.

As you may have already noticed the model that you trained and the
pretrained model share the same overall architecture but they have
different values for some of the parameters:

\texttt{Original\ (pretrained)\ model:}

\begin{verbatim}
TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, 
               dropout=0.1, max_len=4096, ff_activation=tl.Relu)
               
\end{verbatim}

\texttt{Your\ model:}

\begin{verbatim}
TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)
\end{verbatim}

\textbf{Only the parameters shown for your model were changed. The
others stayed the same.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Get the model architecture}
\PY{n}{model} \PY{o}{=} \PY{n}{TransformerLM}\PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Load the pre\PYZhy{}trained weights}
\PY{n}{model}\PY{o}{.}\PY{n}{init\PYZus{}from\PYZus{}file}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model.pkl.gz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{weights\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \# Part 5: Testing with your own input

You will now test your input. You are going to implement greedy
decoding. This consists of two functions. The first one allows you to
identify the next symbol. It gets the argmax of the output of your model
and then returns that index.

\#\#\# Exercise 06 \textbf{Instructions:} Implement the next symbol
function that takes in the cur\_output\_tokens and the trained model to
return the index of the next word.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C9}
\PY{k}{def} \PY{n+nf}{next\PYZus{}symbol}\PY{p}{(}\PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the next symbol for a given sentence.}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        cur\PYZus{}output\PYZus{}tokens (list): tokenized sentence with EOS and PAD tokens at the end.}
\PY{l+s+sd}{        model (trax.layers.combinators.Serial): The transformer model.}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        int: tokenized symbol.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} current output tokens length}
    \PY{n}{token\PYZus{}length} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} calculate the minimum power of 2 big enough to store token\PYZus{}length}
    \PY{c+c1}{\PYZsh{} HINT: use np.ceil() and np.log2()}
    \PY{c+c1}{\PYZsh{} add 1 to token\PYZus{}length so np.log2() doesn\PYZsq{}t receive 0 when token\PYZus{}length is 0}
    \PY{n}{padded\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{2}\PY{o}{*}\PY{o}{*}\PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log2}\PY{p}{(}\PY{n}{token\PYZus{}length} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Fill cur\PYZus{}output\PYZus{}tokens with 0\PYZsq{}s until it reaches padded\PYZus{}length}
    \PY{n}{padded} \PY{o}{=} \PY{n}{cur\PYZus{}output\PYZus{}tokens} \PY{o}{+} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{padded\PYZus{}length} \PY{o}{\PYZhy{}} \PY{n}{token\PYZus{}length}\PY{p}{)}
    \PY{n}{padded\PYZus{}with\PYZus{}batch} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{padded}\PY{p}{)}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{} Don\PYZsq{}t replace this \PYZsq{}None\PYZsq{}! This is a way of setting the batch dim}

    \PY{c+c1}{\PYZsh{} model expects a tuple containing two padded tensors (with batch)}
    \PY{n}{output}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{p}{(}\PY{n}{padded\PYZus{}with\PYZus{}batch}\PY{p}{,} \PY{n}{padded\PYZus{}with\PYZus{}batch}\PY{p}{)}\PY{p}{)} 
    \PY{c+c1}{\PYZsh{} HINT: output has shape (1, padded\PYZus{}length, vocab\PYZus{}size)}
    \PY{c+c1}{\PYZsh{} To get log\PYZus{}probs you need to index output with 0 in the first dim}
    \PY{c+c1}{\PYZsh{} token\PYZus{}length in the second dim and all of the entries for the last dim.}
    \PY{n}{log\PYZus{}probs} \PY{o}{=} \PY{n}{output}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{token\PYZus{}length}\PY{p}{,} \PY{p}{:}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{log\PYZus{}probs}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test it out!}
\PY{n}{sentence\PYZus{}test\PYZus{}nxt\PYZus{}symbl} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{I want to fly in the sky.}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{detokenize}\PY{p}{(}\PY{p}{[}\PY{n}{next\PYZus{}symbol}\PY{p}{(}\PY{n}{tokenize}\PY{p}{(}\PY{n}{sentence\PYZus{}test\PYZus{}nxt\PYZus{}symbl}\PY{p}{)}\PY{o}{+}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
'The'
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\CharTok{\textquotesingle{}T}\ErrorTok{he}\CharTok{\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

    \#\#\# 5.1 Greedy decoding

Now you will implement the greedy\_decode algorithm that will call the
\texttt{next\_symbol} function. It takes in the input\_sentence, the
trained model and returns the decoded sentence.

\#\#\# Exercise 07

\textbf{Instructions}: Implement the greedy\_decode algorithm.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C10}
\PY{c+c1}{\PYZsh{} Decoding functions.}
\PY{k}{def} \PY{n+nf}{greedy\PYZus{}decode}\PY{p}{(}\PY{n}{input\PYZus{}sentence}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Greedy decode function.}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        input\PYZus{}sentence (string): a sentence or article.}
\PY{l+s+sd}{        model (trax.layers.combinators.Serial): Transformer model.}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        string: summary of the input.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} Use tokenize()}
    \PY{n}{cur\PYZus{}output\PYZus{}tokens} \PY{o}{=} \PY{n}{tokenize}\PY{p}{(}\PY{n}{input\PYZus{}sentence}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{generated\PYZus{}output} \PY{o}{=} \PY{p}{[}\PY{p}{]} 
    \PY{n}{cur\PYZus{}output} \PY{o}{=} \PY{l+m+mi}{0} 
    \PY{n}{EOS} \PY{o}{=} \PY{l+m+mi}{1} 
    
    \PY{k}{while} \PY{n}{cur\PYZus{}output} \PY{o}{!=} \PY{n}{EOS}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Get next symbol}
        \PY{n}{cur\PYZus{}output} \PY{o}{=} \PY{n}{next\PYZus{}symbol}\PY{p}{(}\PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{,} \PY{n}{model}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Append next symbol to original sentence}
        \PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cur\PYZus{}output}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Append next symbol to generated sentence}
        \PY{n}{generated\PYZus{}output}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cur\PYZus{}output}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{detokenize}\PY{p}{(}\PY{n}{generated\PYZus{}output}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{detokenize}\PY{p}{(}\PY{n}{generated\PYZus{}output}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test it out on a sentence!}
\PY{n}{test\PYZus{}sentence} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.}\PY{l+s+s2}{\PYZdq{}}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{wrapper}\PY{o}{.}\PY{n}{fill}\PY{p}{(}\PY{n}{test\PYZus{}sentence}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{greedy\PYZus{}decode}\PY{p}{(}\PY{n}{test\PYZus{}sentence}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
It was a sunny day when I went to the market to buy some flowers. But
I only found roses, not tulips.

:
: I
: I just
: I just found
: I just found ros
: I just found roses
: I just found roses,
: I just found roses, not
: I just found roses, not tu
: I just found roses, not tulips
: I just found roses, not tulips
: I just found roses, not tulips.
: I just found roses, not tulips.<EOS>
: I just found roses, not tulips.<EOS>
    \end{Verbatim}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{:}
\NormalTok{: I}
\NormalTok{: I just}
\NormalTok{: I just found}
\NormalTok{: I just found ros}
\NormalTok{: I just found roses}
\NormalTok{: I just found roses,}
\NormalTok{: I just found roses, }\KeywordTok{not}
\NormalTok{: I just found roses, }\KeywordTok{not}\NormalTok{ tu}
\NormalTok{: I just found roses, }\KeywordTok{not}\NormalTok{ tulips}
\NormalTok{: I just found roses, }\KeywordTok{not}\NormalTok{ tulips}
\NormalTok{: I just found roses, }\KeywordTok{not}\NormalTok{ tulips.}
\NormalTok{: I just found roses, }\KeywordTok{not}\NormalTok{ tulips.\textless{}EOS\textgreater{}}
\NormalTok{: I just found roses, }\KeywordTok{not}\NormalTok{ tulips.\textless{}EOS\textgreater{}}
\end{Highlighting}
\end{Shaded}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test it out with a whole article!}
\PY{n}{article} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols \PYZhy{} and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Tebowing}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ craze at the school was blocking the hallway and presenting a safety hazard to students.}\PY{l+s+s2}{\PYZdq{}}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{wrapper}\PY{o}{.}\PY{n}{fill}\PY{p}{(}\PY{n}{article}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{greedy\PYZus{}decode}\PY{p}{(}\PY{n}{article}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \textbf{Expected Output:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Jordan}
\NormalTok{Jordan Ful}
\NormalTok{Jordan Fulcol}
\NormalTok{Jordan Fulcoly}
\NormalTok{Jordan Fulcoly,}
\NormalTok{Jordan Fulcoly, Wayne}
\NormalTok{Jordan Fulcoly, Wayne Dre}
\NormalTok{Jordan Fulcoly, Wayne Drexe}
\NormalTok{Jordan Fulcoly, Wayne Drexel}
\NormalTok{Jordan Fulcoly, Wayne Drexel,}
\NormalTok{.}
\NormalTok{.}
\NormalTok{.}

\NormalTok{Final summary:}

\NormalTok{Jordan Fulcoly, Wayne Drexel, Tyler Carroll }\KeywordTok{and}\NormalTok{ Connor Carroll were}
\NormalTok{suspended }\ControlFlowTok{for}\NormalTok{ one day. Four students were suspended }\ControlFlowTok{for}\NormalTok{ one day}
\NormalTok{because they allegedly did }\KeywordTok{not}\NormalTok{ heed to warnings that the }\CharTok{\textquotesingle{}T}\ErrorTok{ebowing}\CharTok{\textquotesingle{}}
\NormalTok{craze was blocking the hallway }\KeywordTok{and}\NormalTok{ presenting a safety hazard to}
\NormalTok{students.\textless{}EOS\textgreater{}}
\end{Highlighting}
\end{Shaded}

    \textbf{Congratulations on finishing this week's assignment!} You did a
lot of work and now you should have a better understanding of the
encoder part of Transformers and how Transformers can be used for text
summarization.

\textbf{Keep it up!}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
