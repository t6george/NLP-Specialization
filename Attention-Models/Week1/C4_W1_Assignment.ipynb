\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{C4\_W1\_Assignment}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{assignment-1-neural-machine-translation}{%
\section{Assignment 1: Neural Machine
Translation}\label{assignment-1-neural-machine-translation}}

Welcome to the first assignment of Course 4. Here, you will build an
English-to-German neural machine translation (NMT) model using Long
Short-Term Memory (LSTM) networks with attention. Machine translation is
an important task in natural language processing and could be useful not
only for translating one language to another but also for word sense
disambiguation (e.g.~determining whether the word ``bank'' refers to the
financial bank, or the land alongside a river). Implementing this using
just a Recurrent Neural Network (RNN) with LSTMs can work for short to
medium length sentences but can result in vanishing gradients for very
long sequences. To solve this, you will be adding an attention mechanism
to allow the decoder to access all relevant parts of the input sentence
regardless of its length. By completing this assignment, you will:

\begin{itemize}
\tightlist
\item
  learn how to preprocess your training and evaluation data
\item
  implement an encoder-decoder system with attention
\item
  understand how attention works
\item
  build the NMT model from scratch using Trax
\item
  generate translations using greedy and Minimum Bayes Risk (MBR)
  decoding \#\# Outline
\item
  Section \ref{1}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{11}
  \item
    Section \ref{12}
  \item
    Section \ref{13}
  \item
    Section \ref{14}
  \item
    Section \ref{15}
  \end{itemize}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{21}
  \item
    Section \ref{22}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex01}
    \item
      Section \ref{ex02}
    \item
      Section \ref{ex03}
    \end{itemize}
  \item
    Section \ref{23}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex04}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{31}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex05}
    \end{itemize}
  \item
    Section \ref{32}
  \item
    Section \ref{33}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{41}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex06}
    \item
      Section \ref{ex07}
    \end{itemize}
  \item
    Section \ref{42}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex08}
    \item
      Section \ref{ex09}
    \item
      Section \ref{ex10}
    \end{itemize}
  \end{itemize}
\end{itemize}

    \# Part 1: Data Preparation

    \#\# 1.1 Importing the Data

We will first start by importing the packages we will use in this
assignment. As in the previous course of this specialization, we will
use the \href{https://github.com/google/trax}{Trax} library created and
maintained by the \href{https://research.google/teams/brain/}{Google
Brain team} to do most of the heavy lifting. It provides submodules to
fetch and process the datasets, as well as build and train the model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{termcolor} \PY{k+kn}{import} \PY{n}{colored}
\PY{k+kn}{import} \PY{n+nn}{random}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k+kn}{import} \PY{n+nn}{trax}
\PY{k+kn}{from} \PY{n+nn}{trax} \PY{k+kn}{import} \PY{n}{layers} \PY{k}{as} \PY{n}{tl}
\PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{fastmath} \PY{k+kn}{import} \PY{n}{numpy} \PY{k}{as} \PY{n}{fastnp}
\PY{k+kn}{from} \PY{n+nn}{trax}\PY{n+nn}{.}\PY{n+nn}{supervised} \PY{k+kn}{import} \PY{n}{training}

\PY{o}{!}pip list \PY{p}{|} grep trax
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
INFO:tensorflow:tokens\_length=568 inputs\_length=512 targets\_length=114
noise\_density=0.15 mean\_noise\_span\_length=3.0
trax                     1.3.4
\textcolor{ansi-yellow}{WARNING: You are using pip version 20.1.1; however, version 20.3.3 is
available.
You should consider upgrading via the '/opt/conda/bin/python3 -m pip install
--upgrade pip' command.}
    \end{Verbatim}

    Next, we will import the dataset we will use to train the model. To meet
the storage constraints in this lab environment, we will just use a
small dataset from \href{http://opus.nlpl.eu/}{Opus}, a growing
collection of translated texts from the web. Particularly, we will get
an English to German translation subset specified as
\texttt{opus/medical} which has medical related texts. If storage is not
an issue, you can opt to get a larger corpus such as the English to
German translation dataset from \href{https://paracrawl.eu/}{ParaCrawl},
a large multi-lingual translation dataset created by the European Union.
Both of these datasets are available via
\href{https://www.tensorflow.org/datasets}{Tensorflow Datasets (TFDS)}
and you can browse through the other available datasets
\href{https://www.tensorflow.org/datasets/catalog/overview}{here}. We
have downloaded the data for you in the \texttt{data/} directory of your
workspace. As you'll see below, you can easily access this dataset from
TFDS with \texttt{trax.data.TFDS}. The result is a python generator
function yielding tuples. Use the \texttt{keys} argument to select what
appears at which position in the tuple. For example,
\texttt{keys=(\textquotesingle{}en\textquotesingle{},\ \textquotesingle{}de\textquotesingle{})}
below will return pairs as (English sentence, German sentence).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Get generator function for the training set}
\PY{c+c1}{\PYZsh{} This will download the train dataset if no data\PYZus{}dir is specified.}
\PY{n}{train\PYZus{}stream\PYZus{}fn} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{TFDS}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{opus/medical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                 \PY{n}{data\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                 \PY{n}{keys}\PY{o}{=}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{en}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{de}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                                 \PY{n}{eval\PYZus{}holdout\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{c+c1}{\PYZsh{} 1\PYZpc{} for eval}
                                 \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Get generator function for the eval set}
\PY{n}{eval\PYZus{}stream\PYZus{}fn} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{TFDS}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{opus/medical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                \PY{n}{data\PYZus{}dir}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                \PY{n}{keys}\PY{o}{=}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{en}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{de}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                                \PY{n}{eval\PYZus{}holdout\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{c+c1}{\PYZsh{} 1\PYZpc{} for eval}
                                \PY{n}{train}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Notice that TFDS returns a generator \emph{function}, not a generator.
This is because in Python, you cannot reset generators so you cannot go
back to a previously yielded value. During deep learning training, you
use Stochastic Gradient Descent and don't actually need to go back --
but it is sometimes good to be able to do that, and that's where the
functions come in. It is actually very common to use generator functions
in Python -- e.g., \texttt{zip} is a generator function. You can read
more about
\href{https://book.pythontips.com/en/latest/generators.html}{Python
generators} to understand why we use them. Let's print a a sample pair
from our train and eval data. Notice that the raw ouput is represented
in bytes (denoted by the \texttt{b\textquotesingle{}} prefix) and these
will be converted to strings internally in the next steps.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}stream} \PY{o}{=} \PY{n}{train\PYZus{}stream\PYZus{}fn}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train data (en, de) tuple:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n+nb}{next}\PY{p}{(}\PY{n}{train\PYZus{}stream}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{p}{)}

\PY{n}{eval\PYZus{}stream} \PY{o}{=} \PY{n}{eval\PYZus{}stream\PYZus{}fn}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eval data (en, de) tuple:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n+nb}{next}\PY{p}{(}\PY{n}{eval\PYZus{}stream}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-red}{train data (en, de) tuple:} (b'Sverige BRISTOL-MYERS SQUIBB AB Tel: + 46
8 704 71 00\textbackslash{}n', b'Sverige BRISTOL-MYERS SQUIBB AB Tel: + 46 8 704 71 00\textbackslash{}n')

\textcolor{ansi-red}{eval data (en, de) tuple:} (b'Lutropin alfa Subcutaneous use.\textbackslash{}n',
b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\textbackslash{}n')
    \end{Verbatim}

    \#\# 1.2 Tokenization and Formatting

Now that we have imported our corpus, we will be preprocessing the
sentences into a format that our model can accept. This will be composed
of several steps:

\textbf{Tokenizing the sentences using subword representations:} As
you've learned in the earlier courses of this specialization, we want to
represent each sentence as an array of integers instead of strings. For
our application, we will use \emph{subword} representations to tokenize
our sentences. This is a common technique to avoid out-of-vocabulary
words by allowing parts of words to be represented separately. For
example, instead of having separate entries in your vocabulary for
--``fear'', ``fearless'', ``fearsome'', ``some'', and ``less''--, you
can simply store --``fear'', ``some'', and ``less''-- then allow your
tokenizer to combine these subwords when needed. This allows it to be
more flexible so you won't have to save uncommon words explicitly in
your vocabulary (e.g.~\emph{stylebender}, \emph{nonce}, etc). Tokenizing
is done with the \texttt{trax.data.Tokenize()} command and we have
provided you the combined subword vocabulary for English and German
(i.e.~\texttt{ende\_32k.subword}) saved in the \texttt{data} directory.
Feel free to open this file to see how the subwords look like.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} global variables that state the filename and directory of the vocabulary file}
\PY{n}{VOCAB\PYZus{}FILE} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ende\PYZus{}32k.subword}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{VOCAB\PYZus{}DIR} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{} Tokenize the dataset.}
\PY{n}{tokenized\PYZus{}train\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Tokenize}\PY{p}{(}\PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{(}\PY{n}{train\PYZus{}stream}\PY{p}{)}
\PY{n}{tokenized\PYZus{}eval\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Tokenize}\PY{p}{(}\PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{(}\PY{n}{eval\PYZus{}stream}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \textbf{Append an end-of-sentence token to each sentence:} We will
assign a token (i.e.~in this case \texttt{1}) to mark the end of a
sentence. This will be useful in inference/prediction so we'll know that
the model has completed the translation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Append EOS at the end of each sentence.}

\PY{c+c1}{\PYZsh{} Integer assigned as end\PYZhy{}of\PYZhy{}sentence (EOS)}
\PY{n}{EOS} \PY{o}{=} \PY{l+m+mi}{1}

\PY{c+c1}{\PYZsh{} generator helper function to append EOS to each sentence}
\PY{k}{def} \PY{n+nf}{append\PYZus{}eos}\PY{p}{(}\PY{n}{stream}\PY{p}{)}\PY{p}{:}
    \PY{k}{for} \PY{p}{(}\PY{n}{inputs}\PY{p}{,} \PY{n}{targets}\PY{p}{)} \PY{o+ow}{in} \PY{n}{stream}\PY{p}{:}
        \PY{n}{inputs\PYZus{}with\PYZus{}eos} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{inputs}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{n}{EOS}\PY{p}{]}
        \PY{n}{targets\PYZus{}with\PYZus{}eos} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{targets}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{n}{EOS}\PY{p}{]}
        \PY{k}{yield} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{inputs\PYZus{}with\PYZus{}eos}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{targets\PYZus{}with\PYZus{}eos}\PY{p}{)}

\PY{c+c1}{\PYZsh{} append EOS to the train data}
\PY{n}{tokenized\PYZus{}train\PYZus{}stream} \PY{o}{=} \PY{n}{append\PYZus{}eos}\PY{p}{(}\PY{n}{tokenized\PYZus{}train\PYZus{}stream}\PY{p}{)}

\PY{c+c1}{\PYZsh{} append EOS to the eval data}
\PY{n}{tokenized\PYZus{}eval\PYZus{}stream} \PY{o}{=} \PY{n}{append\PYZus{}eos}\PY{p}{(}\PY{n}{tokenized\PYZus{}eval\PYZus{}stream}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \textbf{Filter long sentences:} We will place a limit on the number of
tokens per sentence to ensure we won't run out of memory. This is done
with the \texttt{trax.data.FilterByLength()} method and you can see its
syntax below.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Filter too long sentences to not run out of memory.}
\PY{c+c1}{\PYZsh{} length\PYZus{}keys=[0, 1] means we filter both English and German sentences, so}
\PY{c+c1}{\PYZsh{} both much be not longer that 256 tokens for training / 512 for eval.}
\PY{n}{filtered\PYZus{}train\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{FilterByLength}\PY{p}{(}
    \PY{n}{max\PYZus{}length}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{length\PYZus{}keys}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{(}\PY{n}{tokenized\PYZus{}train\PYZus{}stream}\PY{p}{)}
\PY{n}{filtered\PYZus{}eval\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{FilterByLength}\PY{p}{(}
    \PY{n}{max\PYZus{}length}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{length\PYZus{}keys}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{(}\PY{n}{tokenized\PYZus{}eval\PYZus{}stream}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print a sample input\PYZhy{}target pair of tokenized sentences}
\PY{n}{train\PYZus{}input}\PY{p}{,} \PY{n}{train\PYZus{}target} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{filtered\PYZus{}train\PYZus{}stream}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Single tokenized example input:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}} \PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}input}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Single tokenized example target:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-red}{Single tokenized example input:} [14026  2801  3551 32955   135   150
14443 22008 21980   332 30650  4729
   992     1]
\textcolor{ansi-red}{Single tokenized example target:} [14026  2801  3551 32955   135   150
14443 22008 21980   332 30650  4729
   992     1]
    \end{Verbatim}

    \#\# 1.3 tokenize \& detokenize helper functions

Given any data set, you have to be able to map words to their indices,
and indices to their words. The inputs and outputs to your trax models
are usually tensors of numbers where each number corresponds to a word.
If you were to process your data manually, you would have to make use of
the following:

\begin{itemize}
\tightlist
\item
  { word2Ind: } a dictionary mapping the word to its index.
\item
  { ind2Word:} a dictionary mapping the index to its word.
\item
  { word2Count:} a dictionary mapping the word to the number of times it
  appears.
\item
  { num\_words:} total number of words that have appeared.
\end{itemize}

Since you have already implemented these in previous assignments of the
specialization, we will provide you with helper functions that will do
this for you. Run the cell below to get the following functions:

\begin{itemize}
\tightlist
\item
  { tokenize(): } converts a text sentence to its corresponding token
  list (i.e.~list of indices). Also converts words to subwords (parts of
  words).
\item
  { detokenize(): } converts a token list to its corresponding sentence
  (i.e.~string).
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Setup helper functions for tokenizing and detokenizing sentences}

\PY{k}{def} \PY{n+nf}{tokenize}\PY{p}{(}\PY{n}{input\PYZus{}str}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Encodes a string to an array of integers}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        input\PYZus{}str (str): human\PYZhy{}readable string to encode}
\PY{l+s+sd}{        vocab\PYZus{}file (str): filename of the vocabulary text file}
\PY{l+s+sd}{        vocab\PYZus{}dir (str): path to the vocabulary file}
\PY{l+s+sd}{  }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        numpy.ndarray: tokenized version of the input string}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} Set the encoding of the \PYZdq{}end of sentence\PYZdq{} as 1}
    \PY{n}{EOS} \PY{o}{=} \PY{l+m+mi}{1}
    
    \PY{c+c1}{\PYZsh{} Use the trax.data.tokenize method. It takes streams and returns streams,}
    \PY{c+c1}{\PYZsh{} we get around it by making a 1\PYZhy{}element stream with `iter`.}
    \PY{n}{inputs} \PY{o}{=}  \PY{n+nb}{next}\PY{p}{(}\PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{tokenize}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{p}{[}\PY{n}{input\PYZus{}str}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                                      \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{vocab\PYZus{}dir}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Mark the end of the sentence with EOS}
    \PY{n}{inputs} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{inputs}\PY{p}{)} \PY{o}{+} \PY{p}{[}\PY{n}{EOS}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Adding the batch dimension to the front of the shape}
    \PY{n}{batch\PYZus{}inputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{batch\PYZus{}inputs}


\PY{k}{def} \PY{n+nf}{detokenize}\PY{p}{(}\PY{n}{integers}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Decodes an array of integers to a human readable string}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        integers (numpy.ndarray): array of integers to decode}
\PY{l+s+sd}{        vocab\PYZus{}file (str): filename of the vocabulary text file}
\PY{l+s+sd}{        vocab\PYZus{}dir (str): path to the vocabulary file}
\PY{l+s+sd}{  }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        str: the decoded sentence.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} Remove the dimensions of size 1}
    \PY{n}{integers} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{n}{integers}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Set the encoding of the \PYZdq{}end of sentence\PYZdq{} as 1}
    \PY{n}{EOS} \PY{o}{=} \PY{l+m+mi}{1}
    
    \PY{c+c1}{\PYZsh{} Remove the EOS to decode only the original tokens}
    \PY{k}{if} \PY{n}{EOS} \PY{o+ow}{in} \PY{n}{integers}\PY{p}{:}
        \PY{n}{integers} \PY{o}{=} \PY{n}{integers}\PY{p}{[}\PY{p}{:}\PY{n}{integers}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{n}{EOS}\PY{p}{)}\PY{p}{]} 
    
    \PY{k}{return} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{detokenize}\PY{p}{(}\PY{n}{integers}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{vocab\PYZus{}dir}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Let's see how we might use these functions:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} As declared earlier:}
\PY{c+c1}{\PYZsh{} VOCAB\PYZus{}FILE = \PYZsq{}ende\PYZus{}32k.subword\PYZsq{}}
\PY{c+c1}{\PYZsh{} VOCAB\PYZus{}DIR = \PYZsq{}data/\PYZsq{}}

\PY{c+c1}{\PYZsh{} Detokenize an input\PYZhy{}target pair of tokenized sentences}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Single detokenized example input:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{detokenize}\PY{p}{(}\PY{n}{train\PYZus{}input}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Single detokenized example target:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{detokenize}\PY{p}{(}\PY{n}{train\PYZus{}target}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.}
\PY{c+c1}{\PYZsh{} See how it combines the subwords \PYZhy{}\PYZhy{} \PYZsq{}hell\PYZsq{} and \PYZsq{}o\PYZsq{}\PYZhy{}\PYZhy{} to form the word \PYZsq{}hello\PYZsq{}.}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tokenize(}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{hello}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{): }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{tokenize}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hello}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{detokenize([17332, 140, 1]): }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{detokenize}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{17332}\PY{p}{,} \PY{l+m+mi}{140}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-red}{Single detokenized example input:} Tel: +421 2 57 103 777

\textcolor{ansi-red}{Single detokenized example target:} Tel: +421 2 57 103 777


\textcolor{ansi-green}{tokenize('hello'): } [[17332   140     1]]
\textcolor{ansi-green}{detokenize([17332, 140, 1]): } hello
    \end{Verbatim}

    \#\# 1.4 Bucketing

Bucketing the tokenized sentences is an important technique used to
speed up training in NLP. Here is a
\href{https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976}{nice
article describing it in detail} but the gist is very simple. Our inputs
have variable lengths and you want to make these the same when batching
groups of sentences together. One way to do that is to pad each sentence
to the length of the longest sentence in the dataset. This might lead to
some wasted computation though. For example, if there are multiple short
sentences with just two tokens, do we want to pad these when the longest
sentence is composed of a 100 tokens? Instead of padding with 0s to the
maximum length of a sentence each time, we can group our tokenized
sentences by length and bucket, as on this image (from the article
above):

\begin{figure}
\centering
\includegraphics{https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png}
\caption{alt text}
\end{figure}

We batch the sentences with similar length together (e.g.~the blue
sentences in the image above) and only add minimal padding to make them
have equal length (usually up to the nearest power of two). This allows
to waste less computation when processing padded sequences. In Trax, it
is implemented in the
\href{https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py\#L378}{bucket\_by\_length}
function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Bucketing to create streams of batches.}

\PY{c+c1}{\PYZsh{} Buckets are defined in terms of boundaries and batch sizes.}
\PY{c+c1}{\PYZsh{} Batch\PYZus{}sizes[i] determines the batch size for items with length \PYZlt{} boundaries[i]}
\PY{c+c1}{\PYZsh{} So below, we\PYZsq{}ll take a batch of 256 sentences of length \PYZlt{} 8, 128 if length is}
\PY{c+c1}{\PYZsh{} between 8 and 16, and so on \PYZhy{}\PYZhy{} and only 2 if length is over 512.}
\PY{n}{boundaries} \PY{o}{=}  \PY{p}{[}\PY{l+m+mi}{8}\PY{p}{,}   \PY{l+m+mi}{16}\PY{p}{,}  \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{128}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{512}\PY{p}{]}
\PY{n}{batch\PYZus{}sizes} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{128}\PY{p}{,} \PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{16}\PY{p}{,}    \PY{l+m+mi}{8}\PY{p}{,}   \PY{l+m+mi}{4}\PY{p}{,}  \PY{l+m+mi}{2}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Create the generators.}
\PY{n}{train\PYZus{}batch\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{BucketByLength}\PY{p}{(}
    \PY{n}{boundaries}\PY{p}{,} \PY{n}{batch\PYZus{}sizes}\PY{p}{,}
    \PY{n}{length\PYZus{}keys}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}  \PY{c+c1}{\PYZsh{} As before: count inputs and targets to length.}
\PY{p}{)}\PY{p}{(}\PY{n}{filtered\PYZus{}train\PYZus{}stream}\PY{p}{)}

\PY{n}{eval\PYZus{}batch\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{BucketByLength}\PY{p}{(}
    \PY{n}{boundaries}\PY{p}{,} \PY{n}{batch\PYZus{}sizes}\PY{p}{,}
    \PY{n}{length\PYZus{}keys}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}  \PY{c+c1}{\PYZsh{} As before: count inputs and targets to length.}
\PY{p}{)}\PY{p}{(}\PY{n}{filtered\PYZus{}eval\PYZus{}stream}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Add masking for the padding (0s).}
\PY{n}{train\PYZus{}batch\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AddLossWeights}\PY{p}{(}\PY{n}{id\PYZus{}to\PYZus{}mask}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{(}\PY{n}{train\PYZus{}batch\PYZus{}stream}\PY{p}{)}
\PY{n}{eval\PYZus{}batch\PYZus{}stream} \PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{AddLossWeights}\PY{p}{(}\PY{n}{id\PYZus{}to\PYZus{}mask}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{(}\PY{n}{eval\PYZus{}batch\PYZus{}stream}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \#\# 1.5 Exploring the data

We will now be displaying some of our data. You will see that the
functions defined above (i.e.~\texttt{tokenize()} and
\texttt{detokenize()}) do the same things you have been doing again and
again throughout the specialization. We gave these so you can focus more
on building the model from scratch. Let us first get the data generator
and get one batch of the data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{input\PYZus{}batch}\PY{p}{,} \PY{n}{target\PYZus{}batch}\PY{p}{,} \PY{n}{mask\PYZus{}batch} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n}{train\PYZus{}batch\PYZus{}stream}\PY{p}{)}

\PY{c+c1}{\PYZsh{} let\PYZsq{}s see the data type of a batch}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{input\PYZus{}batch data type: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{type}\PY{p}{(}\PY{n}{input\PYZus{}batch}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{target\PYZus{}batch data type: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{type}\PY{p}{(}\PY{n}{target\PYZus{}batch}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} let\PYZsq{}s see the shape of this particular batch (batch length, sentence length)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{input\PYZus{}batch shape: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{input\PYZus{}batch}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{target\PYZus{}batch shape: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{target\PYZus{}batch}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
input\_batch data type:  <class 'numpy.ndarray'>
target\_batch data type:  <class 'numpy.ndarray'>
input\_batch shape:  (32, 64)
target\_batch shape:  (32, 64)
    \end{Verbatim}

    The \texttt{input\_batch} and \texttt{target\_batch} are Numpy arrays
consisting of tokenized English sentences and German sentences
respectively. These tokens will later be used to produce embedding
vectors for each word in the sentence (so the embedding for a sentence
will be a matrix). The number of sentences in each batch is usually a
power of 2 for optimal computer memory usage.

We can now visually inspect some of the data. You can run the cell below
several times to shuffle through the sentences. Just to note, while this
is a standard data set that is used widely, it does have some known
wrong translations. With that, let's pick a random sentence and print
its tokenized representation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} pick a random index less than the batch size.}
\PY{n}{index} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{randrange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{input\PYZus{}batch}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} use the index to grab an entry from the input and target batch}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{THIS IS THE ENGLISH SENTENCE: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{detokenize}\PY{p}{(}\PY{n}{input\PYZus{}batch}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{input\PYZus{}batch}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{THIS IS THE GERMAN TRANSLATION: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{detokenize}\PY{p}{(}\PY{n}{target\PYZus{}batch}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{colored}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{n}{target\PYZus{}batch}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-red}{THIS IS THE ENGLISH SENTENCE:
} Do not put the pre-filled pen next to the freezer compartment of your
refrigerator or a freezer pack.


\textcolor{ansi-red}{THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE:
 } [ 2937    48   604     4  2960    15  7797 12088   473     9     4 20216
    70 13814  8689  1389     7   139  4773 16766 22939    76    66    13
 20216    70  8781     5  3550 30650  4729   992     1     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0]

\textcolor{ansi-red}{THIS IS THE GERMAN TRANSLATION:
} Legen Sie den Fertigpen nicht in die Nähe des Gefrierfachs oder eines
Kühlelements.


\textcolor{ansi-red}{THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION:
} [25593     5    67    20 14297 12088    44     6    10  1487    38  8542
 10079  9567    14    97   225 21417  8863  6859  7347  3550 30650  4729
   992     1     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0]

    \end{Verbatim}

    \# Part 2: Neural Machine Translation with Attention

Now that you have the data generators and have handled the
preprocessing, it is time for you to build the model. You will be
implementing a neural machine translation model from scratch with
attention.

    \#\# 2.1 Attention Overview

The model we will be building uses an encoder-decoder architecture. This
Recurrent Neural Network (RNN) will take in a tokenized version of a
sentence in its encoder, then passes it on to the decoder for
translation. As mentioned in the lectures, just using a a regular
sequence-to-sequence model with LSTMs will work effectively for short to
medium sentences but will start to degrade for longer ones. You can
picture it like the figure below where all of the context of the input
sentence is compressed into one vector that is passed into the decoder
block. You can see how this will be an issue for very long sentences
(e.g.~100 tokens or more) because the context of the first parts of the
input will have very little effect on the final vector passed to the
decoder.

Adding an attention layer to this model avoids this problem by giving
the decoder access to all parts of the input sentence. To illustrate,
let's just use a 4-word input sentence as shown below. Remember that a
hidden state is produced at each timestep of the encoder (represented by
the orange rectangles). These are all passed to the attention layer and
each are given a score given the current activation (i.e.~hidden state)
of the decoder. For instance, let's consider the figure below where the
first prediction ``Wie'' is already made. To produce the next
prediction, the attention layer will first receive all the encoder
hidden states (i.e.~orange rectangles) as well as the decoder hidden
state when producing the word ``Wie'' (i.e.~first green rectangle).
Given these information, it will score each of the encoder hidden states
to know which one the decoder should focus on to produce the next word.
The result of the model training might have learned that it should align
to the second encoder hidden state and subsequently assigns a high
probability to the word ``geht''. If we are using greedy decoding, we
will output the said word as the next symbol, then restart the process
to produce the next word until we reach an end-of-sentence prediction.

There are different ways to implement attention and the one we'll use
for this assignment is the Scaled Dot Product Attention which has the
form:

\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]

You will dive deeper into this equation in the next week but for now,
you can think of it as computing scores using queries (Q) and keys (K),
followed by a multiplication of values (V) to get a context vector at a
particular timestep of the decoder. This context vector is fed to the
decoder RNN to get a set of probabilities for the next predicted word.
The division by square root of the keys dimensionality (\(\sqrt{d_k}\))
is for improving model performance and you'll also learn more about it
next week. For our machine translation application, the encoder
activations (i.e.~encoder hidden states) will be the keys and values,
while the decoder activations (i.e.~decoder hidden states) will be the
queries.

You will see in the upcoming sections that this complex architecture and
mechanism can be implemented with just a few lines of code. Let's get
started!

    \#\# 2.2 Helper functions

We will first implement a few functions that we will use later on. These
will be for the input encoder, pre-attention decoder, and preparation of
the queries, keys, values, and mask.

\hypertarget{input-encoder}{%
\subsubsection{2.2.1 Input encoder}\label{input-encoder}}

The input encoder runs on the input tokens, creates its embeddings, and
feeds it to an LSTM network. This outputs the activations that will be
the keys and values for attention. It is a
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Serial}{Serial}
network which uses:

\begin{itemize}
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Embedding}{tl.Embedding}:
  Converts each token to its vector representation. In this case, it is
  the the size of the vocabulary by the dimension of the model:
  \texttt{tl.Embedding(vocab\_size,\ d\_model)}. \texttt{vocab\_size} is
  the number of entries in the given vocabulary. \texttt{d\_model} is
  the number of elements in the word embedding.
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.rnn.LSTM}{tl.LSTM}:
  LSTM layer of size \texttt{d\_model}. We want to be able to configure
  how many encoder layers we have so remember to create LSTM layers
  equal to the number of the \texttt{n\_encoder\_layers} parameter.
\end{itemize}

\#\#\# Exercise 01

\textbf{Instructions:} Implement the \texttt{input\_encoder\_fn}
function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{input\PYZus{}encoder\PYZus{}fn}\PY{p}{(}\PY{n}{input\PYZus{}vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{n\PYZus{}encoder\PYZus{}layers}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Input encoder runs on the input sentence and creates}
\PY{l+s+sd}{    activations that will be the keys and values for attention.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        input\PYZus{}vocab\PYZus{}size: int: vocab size of the input}
\PY{l+s+sd}{        d\PYZus{}model: int:  depth of embedding (n\PYZus{}units in the LSTM cell)}
\PY{l+s+sd}{        n\PYZus{}encoder\PYZus{}layers: int: number of LSTM layers in the encoder}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        tl.Serial: The input encoder}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} create a serial network}
    \PY{n}{input\PYZus{}encoder} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(} 
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} create an embedding layer to convert tokens to vectors}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{input\PYZus{}vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,}
        
        \PY{c+c1}{\PYZsh{} feed the embeddings to the LSTM layers. It is a stack of n\PYZus{}encoder\PYZus{}layers LSTM layers}
        \PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{LSTM}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}encoder\PYZus{}layers}\PY{p}{)}\PY{p}{]}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{p}{)}

    \PY{k}{return} \PY{n}{input\PYZus{}encoder}
\end{Verbatim}
\end{tcolorbox}

    \emph{Note: To make this notebook more neat, we moved the unit tests to
a separate file called \texttt{w1\_unittest.py}. Feel free to open it
from your workspace if needed. Just click \texttt{File} on the upper
left corner of this page then \texttt{Open} to see your Jupyter
workspace directory. From there, you can see \texttt{w1\_unittest.py}
and you can open it in another tab or download to see the unit tests. We
have placed comments in that file to indicate which functions are
testing which part of the assignment
(e.g.~\texttt{test\_input\_encoder\_fn()} has the unit tests for
UNQ\_C1).}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{k+kn}{import} \PY{n+nn}{w1\PYZus{}unittest}

\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}input\PYZus{}encoder\PYZus{}fn}\PY{p}{(}\PY{n}{input\PYZus{}encoder\PYZus{}fn}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    \hypertarget{pre-attention-decoder}{%
\subsubsection{2.2.2 Pre-attention
decoder}\label{pre-attention-decoder}}

The pre-attention decoder runs on the targets and creates activations
that are used as queries in attention. This is a Serial network which is
composed of the following:

\begin{itemize}
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.attention.ShiftRight}{tl.ShiftRight}:
  This pads a token to the beginning of your target tokens
  (e.g.~\texttt{{[}8,\ 34,\ 12{]}} shifted right is
  \texttt{{[}0,\ 8,\ 34,\ 12{]}}). This will act like a
  start-of-sentence token that will be the first input to the decoder.
  During training, this shift also allows the target tokens to be passed
  as input to do teacher forcing.
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Embedding}{tl.Embedding}:
  Like in the previous function, this converts each token to its vector
  representation. In this case, it is the the size of the vocabulary by
  the dimension of the model:
  \texttt{tl.Embedding(vocab\_size,\ d\_model)}. \texttt{vocab\_size} is
  the number of entries in the given vocabulary. \texttt{d\_model} is
  the number of elements in the word embedding.
\item
  \href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.rnn.LSTM}{tl.LSTM}:
  LSTM layer of size \texttt{d\_model}.
\end{itemize}

\#\#\# Exercise 02

\textbf{Instructions:} Implement the
\texttt{pre\_attention\_decoder\_fn} function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{pre\PYZus{}attention\PYZus{}decoder\PYZus{}fn}\PY{p}{(}\PY{n}{mode}\PY{p}{,} \PY{n}{target\PYZus{}vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Pre\PYZhy{}attention decoder runs on the targets and creates}
\PY{l+s+sd}{    activations that are used as queries in attention.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        mode: str: \PYZsq{}train\PYZsq{} or \PYZsq{}eval\PYZsq{}}
\PY{l+s+sd}{        target\PYZus{}vocab\PYZus{}size: int: vocab size of the target}
\PY{l+s+sd}{        d\PYZus{}model: int:  depth of embedding (n\PYZus{}units in the LSTM cell)}
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        tl.Serial: The pre\PYZhy{}attention decoder}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} create a serial network}
    \PY{n}{pre\PYZus{}attention\PYZus{}decoder} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} shift right to insert start\PYZhy{}of\PYZhy{}sentence token and implement}
        \PY{c+c1}{\PYZsh{} teacher forcing during training}
        \PY{n}{tl}\PY{o}{.}\PY{n}{ShiftRight}\PY{p}{(}\PY{p}{)}\PY{p}{,}

        \PY{c+c1}{\PYZsh{} run an embedding layer to convert tokens to vectors}
        \PY{n}{tl}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{target\PYZus{}vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,}

        \PY{c+c1}{\PYZsh{} feed to an LSTM layer}
        \PY{n}{tl}\PY{o}{.}\PY{n}{LSTM}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{p}{)}
    
    \PY{k}{return} \PY{n}{pre\PYZus{}attention\PYZus{}decoder}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}

\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}pre\PYZus{}attention\PYZus{}decoder\PYZus{}fn}\PY{p}{(}\PY{n}{pre\PYZus{}attention\PYZus{}decoder\PYZus{}fn}\PY{p}{)}

\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    \hypertarget{preparing-the-attention-input}{%
\subsubsection{2.2.3 Preparing the attention
input}\label{preparing-the-attention-input}}

This function will prepare the inputs to the attention layer. We want to
take in the encoder and pre-attention decoder activations and assign it
to the queries, keys, and values. In addition, another output here will
be the mask to distinguish real tokens from padding tokens. This mask
will be used internally by Trax when computing the softmax so padding
tokens will not have an effect on the computated probabilities. From the
data preparation steps in Section 1 of this assignment, you should know
which tokens in the input correspond to padding.

We have filled the last two lines in composing the mask for you because
it includes a concept that will be discussed further next week. This is
related to \emph{multiheaded attention} which you can think of right now
as computing the attention multiple times to improve the model's
predictions. It is required to consider this additional axis in the
output so we've included it already but you don't need to analyze it
just yet. What's important now is for you to know which should be the
queries, keys, and values, as well as to initialize the mask.

\#\#\# Exercise 03

\textbf{Instructions:} Implement the \texttt{prepare\_attention\_input}
function

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C3}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{prepare\PYZus{}attention\PYZus{}input}\PY{p}{(}\PY{n}{encoder\PYZus{}activations}\PY{p}{,} \PY{n}{decoder\PYZus{}activations}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Prepare queries, keys, values and mask for attention.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        encoder\PYZus{}activations fastnp.array(batch\PYZus{}size, padded\PYZus{}input\PYZus{}length, d\PYZus{}model): output from the input encoder}
\PY{l+s+sd}{        decoder\PYZus{}activations fastnp.array(batch\PYZus{}size, padded\PYZus{}input\PYZus{}length, d\PYZus{}model): output from the pre\PYZhy{}attention decoder}
\PY{l+s+sd}{        inputs fastnp.array(batch\PYZus{}size, padded\PYZus{}input\PYZus{}length): padded input tokens}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        queries, keys, values and mask for attention.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} set the keys and values to the encoder activations}
    \PY{n}{keys} \PY{o}{=} \PY{n}{encoder\PYZus{}activations}
    \PY{n}{values} \PY{o}{=} \PY{n}{encoder\PYZus{}activations}

    
    \PY{c+c1}{\PYZsh{} set the queries to the decoder activations}
    \PY{n}{queries} \PY{o}{=} \PY{n}{decoder\PYZus{}activations}
    
    \PY{c+c1}{\PYZsh{} generate the mask to distinguish real tokens from padding}
    \PY{c+c1}{\PYZsh{} hint: inputs is 1 for real tokens and 0 where they are padding}
    \PY{n}{mask} \PY{o}{=} \PY{n}{inputs} \PY{o}{!=} \PY{l+m+mi}{0}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} add axes to the mask for attention heads and decoder length.}
    \PY{n}{mask} \PY{o}{=} \PY{n}{fastnp}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{mask}\PY{p}{,} \PY{p}{(}\PY{n}{mask}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{mask}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} broadcast so mask shape is [batch size, attention heads, decoder\PYZhy{}len, encoder\PYZhy{}len].}
    \PY{c+c1}{\PYZsh{} note: for this assignment, attention heads is set to 1.}
    \PY{n}{mask} \PY{o}{=} \PY{n}{mask} \PY{o}{+} \PY{n}{fastnp}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{decoder\PYZus{}activations}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        
    
    \PY{k}{return} \PY{n}{queries}\PY{p}{,} \PY{n}{keys}\PY{p}{,} \PY{n}{values}\PY{p}{,} \PY{n}{mask}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}prepare\PYZus{}attention\PYZus{}input}\PY{p}{(}\PY{n}{prepare\PYZus{}attention\PYZus{}input}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    \#\# 2.3 Implementation Overview

We are now ready to implement our sequence-to-sequence model with
attention. This will be a Serial network and is illustrated in the
diagram below. It shows the layers you'll be using in Trax and you'll
see that each step can be implemented quite easily with one line
commands. We've placed several links to the documentation for each
relevant layer in the discussion after the figure below.

    \#\#\# Exercise 04 \textbf{Instructions:} Implement the \texttt{NMTAttn}
function below to define your machine translation model which uses
attention. We have left hyperlinks below pointing to the Trax
documentation of the relevant layers. Remember to consult it to get tips
on what parameters to pass.

\textbf{Step 0:} Prepare the input encoder and pre-attention decoder
branches. You have already defined this earlier as helper functions so
it's just a matter of calling those functions and assigning it to
variables.

\textbf{Step 1:} Create a Serial network. This will stack the layers in
the next steps one after the other. Like the earlier exercises, you can
use
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Serial}{tl.Serial}.

\textbf{Step 2:} Make a copy of the input and target tokens. As you see
in the diagram above, the input and target tokens will be fed into
different layers of the model. You can use
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Select}{tl.Select}
layer to create copies of these tokens. Arrange them as
\texttt{{[}input\ tokens,\ target\ tokens,\ input\ tokens,\ target\ tokens{]}}.

\textbf{Step 3:} Create a parallel branch to feed the input tokens to
the \texttt{input\_encoder} and the target tokens to the
\texttt{pre\_attention\_decoder}. You can use
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Parallel}{tl.Parallel}
to create these sublayers in parallel. Remember to pass the variables
you defined in Step 0 as parameters to this layer.

\textbf{Step 4:} Next, call the \texttt{prepare\_attention\_input}
function to convert the encoder and pre-attention decoder activations to
a format that the attention layer will accept. You can use
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.base.Fn}{tl.Fn}
to call this function. Note: Pass the \texttt{prepare\_attention\_input}
function as the \texttt{f} parameter in \texttt{tl.Fn} without any
arguments or parenthesis.

\textbf{Step 5:} We will now feed the (queries, keys, values, and mask)
to the
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.attention.AttentionQKV}{tl.AttentionQKV}
layer. This computes the scaled dot product attention and outputs the
attention weights and mask. Take note that although it is a one liner,
this layer is actually composed of a deep network made up of several
branches. We'll show the implementation taken
\href{https://github.com/google/trax/blob/master/trax/layers/attention.py\#L61}{here}
to see the different layers used.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ AttentionQKV(d\_feature, n\_heads}\OperatorTok{=}\DecValTok{1}\NormalTok{, dropout}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, mode}\OperatorTok{=}\StringTok{\textquotesingle{}train\textquotesingle{}}\NormalTok{):}
  \CommentTok{"""Returns a layer that maps (q, k, v, mask) to (activations, mask).}

\CommentTok{  See \textasciigrave{}Attention\textasciigrave{} above for further context/details.}

\CommentTok{  Args:}
\CommentTok{    d\_feature: Depth/dimensionality of feature embedding.}
\CommentTok{    n\_heads: Number of attention heads.}
\CommentTok{    dropout: Probababilistic rate for internal dropout applied to attention}
\CommentTok{        activations (based on query{-}key pairs) before dotting them with values.}
\CommentTok{    mode: Either \textquotesingle{}train\textquotesingle{} or \textquotesingle{}eval\textquotesingle{}.}
\CommentTok{  """}
  \ControlFlowTok{return}\NormalTok{ cb.Serial(}
\NormalTok{      cb.Parallel(}
\NormalTok{          core.Dense(d\_feature),}
\NormalTok{          core.Dense(d\_feature),}
\NormalTok{          core.Dense(d\_feature),}
\NormalTok{      ),}
\NormalTok{      PureAttention(  }\CommentTok{\# pylint: disable=no{-}value{-}for{-}parameter}
\NormalTok{          n\_heads}\OperatorTok{=}\NormalTok{n\_heads, dropout}\OperatorTok{=}\NormalTok{dropout, mode}\OperatorTok{=}\NormalTok{mode),}
\NormalTok{      core.Dense(d\_feature),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Having deep layers pose the risk of vanishing gradients during training
and we would want to mitigate that. To improve the ability of the
network to learn, we can insert a
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Residual}{tl.Residual}
layer to add the output of AttentionQKV with the \texttt{queries} input.
You can do this in trax by simply nesting the \texttt{AttentionQKV}
layer inside the \texttt{Residual} layer. The library will take care of
branching and adding for you.

\textbf{Step 6:} We will not need the mask for the model we're building
so we can safely drop it. At this point in the network, the signal stack
currently has
\texttt{{[}attention\ activations,\ mask,\ target\ tokens{]}} and you
can use
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.combinators.Select}{tl.Select}
to output just \texttt{{[}attention\ activations,\ target\ tokens{]}}.

\textbf{Step 7:} We can now feed the attention weighted output to the
LSTM decoder. We can stack multiple
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.rnn.LSTM}{tl.LSTM}
layers to improve the output so remember to append LSTMs equal to the
number defined by \texttt{n\_decoder\_layers} parameter to the model.

\textbf{Step 8:} We want to determine the probabilities of each subword
in the vocabulary and you can set this up easily with a
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.Dense}{tl.Dense}
layer by making its size equal to the size of our vocabulary.

\textbf{Step 9:} Normalize the output to log probabilities by passing
the activations in Step 8 to a
\href{https://trax-ml.readthedocs.io/en/latest/trax.layers.html\#trax.layers.core.LogSoftmax}{tl.LogSoftmax}
layer.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C4}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{NMTAttn}\PY{p}{(}\PY{n}{input\PYZus{}vocab\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{33300}\PY{p}{,}
            \PY{n}{target\PYZus{}vocab\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{33300}\PY{p}{,}
            \PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{1024}\PY{p}{,}
            \PY{n}{n\PYZus{}encoder\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
            \PY{n}{n\PYZus{}decoder\PYZus{}layers}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
            \PY{n}{n\PYZus{}attention\PYZus{}heads}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,}
            \PY{n}{attention\PYZus{}dropout}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,}
            \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns an LSTM sequence\PYZhy{}to\PYZhy{}sequence model with attention.}

\PY{l+s+sd}{    The input to the model is a pair (input tokens, target tokens), e.g.,}
\PY{l+s+sd}{    an English sentence (tokenized) and its translation into German (tokenized).}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{    input\PYZus{}vocab\PYZus{}size: int: vocab size of the input}
\PY{l+s+sd}{    target\PYZus{}vocab\PYZus{}size: int: vocab size of the target}
\PY{l+s+sd}{    d\PYZus{}model: int:  depth of embedding (n\PYZus{}units in the LSTM cell)}
\PY{l+s+sd}{    n\PYZus{}encoder\PYZus{}layers: int: number of LSTM layers in the encoder}
\PY{l+s+sd}{    n\PYZus{}decoder\PYZus{}layers: int: number of LSTM layers in the decoder after attention}
\PY{l+s+sd}{    n\PYZus{}attention\PYZus{}heads: int: number of attention heads}
\PY{l+s+sd}{    attention\PYZus{}dropout: float, dropout for the attention layer}
\PY{l+s+sd}{    mode: str: \PYZsq{}train\PYZsq{}, \PYZsq{}eval\PYZsq{} or \PYZsq{}predict\PYZsq{}, predict mode is for fast inference}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    A LSTM sequence\PYZhy{}to\PYZhy{}sequence model with attention.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} Step 0: call the helper function to create layers for the input encoder}
    \PY{n}{input\PYZus{}encoder} \PY{o}{=} \PY{n}{input\PYZus{}encoder\PYZus{}fn}\PY{p}{(}\PY{n}{input\PYZus{}vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{n\PYZus{}encoder\PYZus{}layers}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Step 0: call the helper function to create layers for the pre\PYZhy{}attention decoder}
    \PY{n}{pre\PYZus{}attention\PYZus{}decoder} \PY{o}{=} \PY{n}{pre\PYZus{}attention\PYZus{}decoder\PYZus{}fn}\PY{p}{(}\PY{n}{mode}\PY{p}{,} \PY{n}{target\PYZus{}vocab\PYZus{}size}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Step 1: create a serial network}
    \PY{n}{model} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Serial}\PY{p}{(} 
        
      \PY{c+c1}{\PYZsh{} Step 2: copy input tokens and target tokens as they will be needed later.}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Select}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        
      \PY{c+c1}{\PYZsh{} Step 3: run input encoder on the input and pre\PYZhy{}attention decoder the target.}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Parallel}\PY{p}{(}\PY{n}{input\PYZus{}encoder}\PY{p}{,} \PY{n}{pre\PYZus{}attention\PYZus{}decoder}\PY{p}{)}\PY{p}{,}
        
      \PY{c+c1}{\PYZsh{} Step 4: prepare queries, keys, values and mask for attention.}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Fn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PrepareAttentionInput}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{prepare\PYZus{}attention\PYZus{}input}\PY{p}{,} \PY{n}{n\PYZus{}out}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
        
      \PY{c+c1}{\PYZsh{} Step 5: run the AttentionQKV layer}
      \PY{c+c1}{\PYZsh{} nest it inside a Residual layer to add to the pre\PYZhy{}attention decoder activations(i.e. queries)}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Residual}\PY{p}{(}\PY{n}{tl}\PY{o}{.}\PY{n}{AttentionQKV}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{n\PYZus{}heads}\PY{o}{=}\PY{n}{n\PYZus{}attention\PYZus{}heads}\PY{p}{,} \PY{n}{dropout}\PY{o}{=}\PY{n}{attention\PYZus{}dropout}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{n}{mode}\PY{p}{)}\PY{p}{)}\PY{p}{,}
      
      \PY{c+c1}{\PYZsh{} Step 6: drop attention mask (i.e. index = None}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Select}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        
      \PY{c+c1}{\PYZsh{} Step 7: run the rest of the RNN decoder}
      \PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{LSTM}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{)} \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}decoder\PYZus{}layers}\PY{p}{)}\PY{p}{]}\PY{p}{,}
        
      \PY{c+c1}{\PYZsh{} Step 8: prepare output by making it the right size}
      \PY{n}{tl}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{target\PYZus{}vocab\PYZus{}size}\PY{p}{)}\PY{p}{,}
        
      \PY{c+c1}{\PYZsh{} Step 9: Log\PYZhy{}softmax for output}
      \PY{n}{tl}\PY{o}{.}\PY{n}{LogSoftmax}\PY{p}{(}\PY{p}{)}
    \PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE}
    
    \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}NMTAttn}\PY{p}{(}\PY{n}{NMTAttn}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print your model}
\PY{n}{model} \PY{o}{=} \PY{n}{NMTAttn}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Serial\_in2\_out2[
  Select[0,1,0,1]\_in2\_out4
  Parallel\_in2\_out2[
    Serial[
      Embedding\_33300\_1024
      LSTM\_1024
      LSTM\_1024
    ]
    Serial[
      ShiftRight(1)
      Embedding\_33300\_1024
      LSTM\_1024
    ]
  ]
  PrepareAttentionInput\_in3\_out4
  Serial\_in4\_out2[
    Branch\_in4\_out3[
      None
      Serial\_in4\_out2[
        Parallel\_in3\_out3[
          Dense\_1024
          Dense\_1024
          Dense\_1024
        ]
        PureAttention\_in4\_out2
        Dense\_1024
      ]
    ]
    Add\_in2
  ]
  Select[0,2]\_in3\_out2
  LSTM\_1024
  LSTM\_1024
  Dense\_33300
  LogSoftmax
]
    \end{Verbatim}

    \textbf{Expected Output:}

\begin{verbatim}
Serial_in2_out2[
  Select[0,1,0,1]_in2_out4
  Parallel_in2_out2[
    Serial[
      Embedding_33300_1024
      LSTM_1024
      LSTM_1024
    ]
    Serial[
      ShiftRight(1)
      Embedding_33300_1024
      LSTM_1024
    ]
  ]
  PrepareAttentionInput_in3_out4
  Serial_in4_out2[
    Branch_in4_out3[
      None
      Serial_in4_out2[
        Parallel_in3_out3[
          Dense_1024
          Dense_1024
          Dense_1024
        ]
        PureAttention_in4_out2
        Dense_1024
      ]
    ]
    Add_in2
  ]
  Select[0,2]_in3_out2
  LSTM_1024
  LSTM_1024
  Dense_33300
  LogSoftmax
]
\end{verbatim}

    \# Part 3: Training

We will now be training our model in this section. Doing supervised
training in Trax is pretty straightforward (short example
\href{https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html\#Supervised-training}{here}).
We will be instantiating three classes for this: \texttt{TrainTask},
\texttt{EvalTask}, and \texttt{Loop}. Let's take a closer look at each
of these in the sections below.

    \#\# 3.1 TrainTask

The
\href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.training.TrainTask}{TrainTask}
class allows us to define the labeled data to use for training and the
feedback mechanisms to compute the loss and update the weights.

\#\#\# Exercise 05

\textbf{Instructions:} Instantiate a train task.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C5}
\PY{c+c1}{\PYZsh{} GRADED }
\PY{n}{train\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{TrainTask}\PY{p}{(}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} use the train batch stream as labeled data}
    \PY{n}{labeled\PYZus{}data}\PY{o}{=} \PY{n}{train\PYZus{}batch\PYZus{}stream}\PY{p}{,}
    
    \PY{c+c1}{\PYZsh{} use the cross entropy loss}
    \PY{n}{loss\PYZus{}layer}\PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    
    \PY{c+c1}{\PYZsh{} use the Adam optimizer with learning rate of 0.01}
    \PY{n}{optimizer}\PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,}
    
    \PY{c+c1}{\PYZsh{} use the `trax.lr.warmup\PYZus{}and\PYZus{}rsqrt\PYZus{}decay` as the learning rate schedule}
    \PY{c+c1}{\PYZsh{} have 1000 warmup steps with a max value of 0.01}
    \PY{n}{lr\PYZus{}schedule}\PY{o}{=} \PY{n}{trax}\PY{o}{.}\PY{n}{lr}\PY{o}{.}\PY{n}{warmup\PYZus{}and\PYZus{}rsqrt\PYZus{}decay}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,}
    
    \PY{c+c1}{\PYZsh{} have a checkpoint every 10 steps}
    \PY{n}{n\PYZus{}steps\PYZus{}per\PYZus{}checkpoint}\PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}train\PYZus{}task}\PY{p}{(}\PY{n}{train\PYZus{}task}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    \#\# 3.2 EvalTask

The
\href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.training.EvalTask}{EvalTask}
on the other hand allows us to see how the model is doing while
training. For our application, we want it to report the cross entropy
loss and accuracy.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{eval\PYZus{}task} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{EvalTask}\PY{p}{(}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{} use the eval batch stream as labeled data}
    \PY{n}{labeled\PYZus{}data}\PY{o}{=}\PY{n}{eval\PYZus{}batch\PYZus{}stream}\PY{p}{,}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{} use the cross entropy loss and accuracy as metrics}
    \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{n}{tl}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{tl}\PY{o}{.}\PY{n}{Accuracy}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \#\# 3.3 Loop

The
\href{https://trax-ml.readthedocs.io/en/latest/trax.supervised.html\#trax.supervised.training.Loop}{Loop}
class defines the model we will train as well as the train and eval
tasks to execute. Its \texttt{run()} method allows us to execute the
training for a specified number of steps.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} define the output directory}
\PY{n}{output\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{output\PYZus{}dir/}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{} remove old model if it exists. restarts training.}
\PY{o}{!}rm \PYZhy{}f \PYZti{}/output\PYZus{}dir/model.pkl.gz  

\PY{c+c1}{\PYZsh{} define the training loop}
\PY{n}{training\PYZus{}loop} \PY{o}{=} \PY{n}{training}\PY{o}{.}\PY{n}{Loop}\PY{p}{(}\PY{n}{NMTAttn}\PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                              \PY{n}{train\PYZus{}task}\PY{p}{,}
                              \PY{n}{eval\PYZus{}tasks}\PY{o}{=}\PY{p}{[}\PY{n}{eval\PYZus{}task}\PY{p}{]}\PY{p}{,}
                              \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{n}{output\PYZus{}dir}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} NOTE: Execute the training loop. This will take around 8 minutes to complete.}
\PY{n}{training\PYZus{}loop}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Step      1: Ran 1 train steps in 127.43 secs
Step      1: train CrossEntropyLoss |  10.39996433
Step      1: eval  CrossEntropyLoss |  10.40326786
Step      1: eval          Accuracy |  0.00000000

Step     10: Ran 9 train steps in 364.90 secs
Step     10: train CrossEntropyLoss |  10.24230099
Step     10: eval  CrossEntropyLoss |  9.93186665
Step     10: eval          Accuracy |  0.02429765
    \end{Verbatim}

    \# Part 4: Testing

We will now be using the model you just trained to translate English
sentences to German. We will implement this with two functions: The
first allows you to identify the next symbol (i.e.~output token). The
second one takes care of combining the entire translated string.

We will start by first loading in a pre-trained copy of the model you
just coded. Please run the cell below to do just that.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} instantiate the model we built in eval mode}
\PY{n}{model} \PY{o}{=} \PY{n}{NMTAttn}\PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} initialize weights from a pre\PYZhy{}trained model}
\PY{n}{model}\PY{o}{.}\PY{n}{init\PYZus{}from\PYZus{}file}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model.pkl.gz}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{weights\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{model} \PY{o}{=} \PY{n}{tl}\PY{o}{.}\PY{n}{Accelerate}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \#\# 4.1 Decoding

As discussed in the lectures, there are several ways to get the next
token when translating a sentence. For instance, we can just get the
most probable token at each step (i.e.~greedy decoding) or get a sample
from a distribution. We can generalize the implementation of these two
approaches by using the \texttt{tl.logsoftmax\_sample()} method. Let's
briefly look at its implementation:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ logsoftmax\_sample(log\_probs, temperature}\OperatorTok{=}\FloatTok{1.0}\NormalTok{):  }\CommentTok{\# pylint: disable=invalid{-}name}
  \CommentTok{"""Returns a sample from a log{-}softmax output, with temperature.}

\CommentTok{  Args:}
\CommentTok{    log\_probs: Logarithms of probabilities (often coming from LogSofmax)}
\CommentTok{    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)}
\CommentTok{  """}
  \CommentTok{\# This is equivalent to sampling from a softmax with temperature.}
\NormalTok{  u }\OperatorTok{=}\NormalTok{ np.random.uniform(low}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{, high}\OperatorTok{=}\FloatTok{1.0} \OperatorTok{{-}} \FloatTok{1e{-}6}\NormalTok{, size}\OperatorTok{=}\NormalTok{log\_probs.shape)}
\NormalTok{  g }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{np.log(}\OperatorTok{{-}}\NormalTok{np.log(u))}
  \ControlFlowTok{return}\NormalTok{ np.argmax(log\_probs }\OperatorTok{+}\NormalTok{ g }\OperatorTok{*}\NormalTok{ temperature, axis}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The key things to take away here are: 1. it gets random samples with the
same shape as your input (i.e.~\texttt{log\_probs}), and 2. the amount
of ``noise'' added to the input by these random samples is scaled by a
\texttt{temperature} setting. You'll notice that setting it to
\texttt{0} will just make the return statement equal to getting the
argmax of \texttt{log\_probs}. This will come in handy later.

\#\#\# Exercise 06

\textbf{Instructions:} Implement the \texttt{next\_symbol()} function
that takes in the \texttt{input\_tokens} and the
\texttt{cur\_output\_tokens}, then return the index of the next word.
You can click below for hints in completing this exercise.

Click Here for Hints

To get the next power of two, you can compute 2\^{}log\_2(token\_length
+ 1) . We add 1 to avoid log(0).

You can use np.ceil() to get the ceiling of a float.

np.log2() will get the logarithm base 2 of a value

int() will cast a value into an integer type

From the model diagram in part 2, you know that it takes two inputs. You
can feed these with this syntax to get the model outputs: model((input1,
input2)). It's up to you to determine which variables below to
substitute for input1 and input2. Remember also from the diagram that
the output has two elements: {[}log probabilities, target tokens{]}. You
won't need the target tokens so we assigned it to \_ below for you.

The log probabilities output will have the shape: (batch size, decoder
length, vocab size). It will contain log probabilities for each token in
the cur\_output\_tokens plus 1 for the start symbol introduced by the
ShiftRight in the preattention decoder. For example, if
cur\_output\_tokens is {[}1, 2, 5{]}, the model will output an array of
log probabilities each for tokens 0 (start symbol), 1, 2, and 5. To
generate the next symbol, you just want to get the log probabilities
associated with the last token (i.e.~token 5 at index 3). You can slice
the model output at {[}0, 3, :{]} to get this. It will be up to you to
generalize this for any length of cur\_output\_tokens

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C6}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{next\PYZus{}symbol}\PY{p}{(}\PY{n}{NMTAttn}\PY{p}{,} \PY{n}{input\PYZus{}tokens}\PY{p}{,} \PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{,} \PY{n}{temperature}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the index of the next token.}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        NMTAttn (tl.Serial): An LSTM sequence\PYZhy{}to\PYZhy{}sequence model with attention.}
\PY{l+s+sd}{        input\PYZus{}tokens (np.ndarray 1 x n\PYZus{}tokens): tokenized representation of the input sentence}
\PY{l+s+sd}{        cur\PYZus{}output\PYZus{}tokens (list): tokenized representation of previously translated words}
\PY{l+s+sd}{        temperature (float): parameter for sampling ranging from 0.0 to 1.0.}
\PY{l+s+sd}{            0.0: same as argmax, always pick the most probable token}
\PY{l+s+sd}{            1.0: sampling from the distribution (can sometimes say random things)}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        int: index of the next token in the translated sentence}
\PY{l+s+sd}{        float: log probability of the next symbol}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}

    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}

    \PY{c+c1}{\PYZsh{} set the length of the current output tokens}
    \PY{n}{token\PYZus{}length} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} calculate next power of 2 for padding length }
    \PY{n}{padded\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*}\PY{o}{*} \PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log2}\PY{p}{(}\PY{n}{token\PYZus{}length} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} pad cur\PYZus{}output\PYZus{}tokens up to the padded\PYZus{}length}
    \PY{n}{padded} \PY{o}{=} \PY{n}{cur\PYZus{}output\PYZus{}tokens} \PY{o}{+} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{padded\PYZus{}length} \PY{o}{\PYZhy{}} \PY{n}{token\PYZus{}length}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} model expects the output to have an axis for the batch size in front so}
    \PY{c+c1}{\PYZsh{} convert `padded` list to a numpy array with shape (x, \PYZlt{}padded\PYZus{}length\PYZgt{}) where the}
    \PY{c+c1}{\PYZsh{} x position is the batch axis. (hint: you can use np.expand\PYZus{}dims() with axis=0 to insert a new axis)}
    \PY{n}{padded\PYZus{}with\PYZus{}batch} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{padded}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} get the model prediction. remember to use the `NMTAttn` argument defined above.}
    \PY{c+c1}{\PYZsh{} hint: the model accepts a tuple as input (e.g. `my\PYZus{}model((input1, input2))`)}
    \PY{n}{output}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{NMTAttn}\PY{p}{(}\PY{p}{(}\PY{n}{input\PYZus{}tokens}\PY{p}{,} \PY{n}{padded\PYZus{}with\PYZus{}batch}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} get log probabilities from the last token output}
    \PY{n}{log\PYZus{}probs} \PY{o}{=} \PY{n}{output}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{token\PYZus{}length}\PY{p}{,} \PY{p}{:}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} get the next symbol by getting a logsoftmax sample (*hint: cast to an int)}
    \PY{n}{symbol} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{tl}\PY{o}{.}\PY{n}{logsoftmax\PYZus{}sample}\PY{p}{(}\PY{n}{log\PYZus{}probs}\PY{p}{,} \PY{n}{temperature}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}

    \PY{k}{return} \PY{n}{symbol}\PY{p}{,} \PY{n+nb}{float}\PY{p}{(}\PY{n}{log\PYZus{}probs}\PY{p}{[}\PY{n}{symbol}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}next\PYZus{}symbol}\PY{p}{(}\PY{n}{next\PYZus{}symbol}\PY{p}{,} \PY{n}{model}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    Now you will implement the \texttt{sampling\_decode()} function. This
will call the \texttt{next\_symbol()} function above several times until
the next output is the end-of-sentence token (i.e.~\texttt{EOS}). It
takes in an input string and returns the translated version of that
string.

\#\#\# Exercise 07

\textbf{Instructions}: Implement the \texttt{sampling\_decode()}
function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C7}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{sampling\PYZus{}decode}\PY{p}{(}\PY{n}{input\PYZus{}sentence}\PY{p}{,} \PY{n}{NMTAttn} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,} \PY{n}{temperature}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the translated sentence.}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        input\PYZus{}sentence (str): sentence to translate.}
\PY{l+s+sd}{        NMTAttn (tl.Serial): An LSTM sequence\PYZhy{}to\PYZhy{}sequence model with attention.}
\PY{l+s+sd}{        temperature (float): parameter for sampling ranging from 0.0 to 1.0.}
\PY{l+s+sd}{            0.0: same as argmax, always pick the most probable token}
\PY{l+s+sd}{            1.0: sampling from the distribution (can sometimes say random things)}
\PY{l+s+sd}{        vocab\PYZus{}file (str): filename of the vocabulary}
\PY{l+s+sd}{        vocab\PYZus{}dir (str): path to the vocabulary file}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        tuple: (list, str, float)}
\PY{l+s+sd}{            list of int: tokenized version of the translated sentence}
\PY{l+s+sd}{            float: log probability of the translated sentence}
\PY{l+s+sd}{            str: the translated sentence}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} encode the input sentence}
    \PY{n}{input\PYZus{}tokens} \PY{o}{=} \PY{n}{tokenize}\PY{p}{(}\PY{n}{input\PYZus{}sentence}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} initialize the list of output tokens}
    \PY{n}{cur\PYZus{}output\PYZus{}tokens} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} initialize an integer that represents the current output index}
    \PY{n}{cur\PYZus{}output} \PY{o}{=} \PY{l+m+mi}{0}
    
    \PY{c+c1}{\PYZsh{} Set the encoding of the \PYZdq{}end of sentence\PYZdq{} as 1}
    \PY{n}{EOS} \PY{o}{=} \PY{l+m+mi}{1}
    
    \PY{c+c1}{\PYZsh{} check that the current output is not the end of sentence token}
    \PY{k}{while} \PY{n}{cur\PYZus{}output} \PY{o}{!=} \PY{n}{EOS}\PY{p}{:}
        
        \PY{c+c1}{\PYZsh{} update the current output token by getting the index of the next word (hint: use next\PYZus{}symbol)}
        \PY{n}{cur\PYZus{}output}\PY{p}{,} \PY{n}{log\PYZus{}prob} \PY{o}{=} \PY{n}{next\PYZus{}symbol}\PY{p}{(}\PY{n}{NMTAttn}\PY{p}{,} \PY{n}{input\PYZus{}tokens}\PY{p}{,} \PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{,} \PY{n}{temperature}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} append the current output token to the list of output tokens}
        \PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cur\PYZus{}output}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} detokenize the output tokens}
    \PY{n}{sentence} \PY{o}{=} \PY{n}{detokenize}\PY{p}{(}\PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{cur\PYZus{}output\PYZus{}tokens}\PY{p}{,} \PY{n}{log\PYZus{}prob}\PY{p}{,} \PY{n}{sentence}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test the function above. Try varying the temperature setting with values from 0 to 1.}
\PY{c+c1}{\PYZsh{} Run it several times with each setting and see how often the output changes.}
\PY{n}{sampling\PYZus{}decode}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{I love languages.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{temperature}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
([161, 12202, 5112, 3, 1], -0.0001735687255859375, 'Ich liebe Sprachen.')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}sampling\PYZus{}decode}\PY{p}{(}\PY{n}{sampling\PYZus{}decode}\PY{p}{,} \PY{n}{model}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    We have set a default value of \texttt{0} to the temperature setting in
our implementation of \texttt{sampling\_decode()} above. As you may have
noticed in the \texttt{logsoftmax\_sample()} method, this setting will
ultimately result in greedy decoding. As mentioned in the lectures, this
algorithm generates the translation by getting the most probable word at
each step. It gets the argmax of the output array of your model and then
returns that index. See the testing function and sample inputs below.
You'll notice that the output will remain the same each time you run it.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{greedy\PYZus{}decode\PYZus{}test}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{NMTAttn}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Prints the input and output of our NMTAttn model using greedy decode}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        sentence (str): a custom string.}
\PY{l+s+sd}{        NMTAttn (tl.Serial): An LSTM sequence\PYZhy{}to\PYZhy{}sequence model with attention.}
\PY{l+s+sd}{        vocab\PYZus{}file (str): filename of the vocabulary}
\PY{l+s+sd}{        vocab\PYZus{}dir (str): path to the vocabulary file}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        str: the translated sentence}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{\PYZus{}}\PY{p}{,}\PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{translated\PYZus{}sentence} \PY{o}{=} \PY{n}{sampling\PYZus{}decode}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{NMTAttn}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{vocab\PYZus{}dir}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{English: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sentence}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{German: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{translated\PYZus{}sentence}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{translated\PYZus{}sentence}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} put a custom string here}
\PY{n}{your\PYZus{}sentence} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{I love languages.}\PY{l+s+s1}{\PYZsq{}}

\PY{n}{greedy\PYZus{}decode\PYZus{}test}\PY{p}{(}\PY{n}{your\PYZus{}sentence}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
English:  I love languages.
German:  Ich liebe Sprachen.
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{greedy\PYZus{}decode\PYZus{}test}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{You are almost done with the assignment!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
English:  You are almost done with the assignment!
German:  Sie sind fast mit der Aufgabe fertig!
    \end{Verbatim}

    \#\# 4.2 Minimum Bayes-Risk Decoding

As mentioned in the lectures, getting the most probable token at each
step may not necessarily produce the best results. Another approach is
to do Minimum Bayes Risk Decoding or MBR. The general steps to implement
this are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  take several random samples
\item
  score each sample against all other samples
\item
  select the one with the highest score
\end{enumerate}

You will be building helper functions for these steps in the following
sections.

    \#\#\# 4.2.1 Generating samples

First, let's build a function to generate several samples. You can use
the \texttt{sampling\_decode()} function you developed earlier to do
this easily. We want to record the token list and log probability for
each sample as these will be needed in the next step.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{generate\PYZus{}samples}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{NMTAttn}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{temperature}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Generates samples using sampling\PYZus{}decode()}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        sentence (str): sentence to translate.}
\PY{l+s+sd}{        n\PYZus{}samples (int): number of samples to generate}
\PY{l+s+sd}{        NMTAttn (tl.Serial): An LSTM sequence\PYZhy{}to\PYZhy{}sequence model with attention.}
\PY{l+s+sd}{        temperature (float): parameter for sampling ranging from 0.0 to 1.0.}
\PY{l+s+sd}{            0.0: same as argmax, always pick the most probable token}
\PY{l+s+sd}{            1.0: sampling from the distribution (can sometimes say random things)}
\PY{l+s+sd}{        vocab\PYZus{}file (str): filename of the vocabulary}
\PY{l+s+sd}{        vocab\PYZus{}dir (str): path to the vocabulary file}
\PY{l+s+sd}{        }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        tuple: (list, list)}
\PY{l+s+sd}{            list of lists: token list per sample}
\PY{l+s+sd}{            list of floats: log probability per sample}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} define lists to contain samples and probabilities}
    \PY{n}{samples}\PY{p}{,} \PY{n}{log\PYZus{}probs} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} run a for loop to generate n samples}
    \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)}\PY{p}{:}
        
        \PY{c+c1}{\PYZsh{} get a sample using the sampling\PYZus{}decode() function}
        \PY{n}{sample}\PY{p}{,} \PY{n}{logp}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{sampling\PYZus{}decode}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{NMTAttn}\PY{p}{,} \PY{n}{temperature}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{vocab\PYZus{}dir}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} append the token list to the samples list}
        \PY{n}{samples}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sample}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} append the log probability to the log\PYZus{}probs list}
        \PY{n}{log\PYZus{}probs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{logp}\PY{p}{)}
                
    \PY{k}{return} \PY{n}{samples}\PY{p}{,} \PY{n}{log\PYZus{}probs}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} generate 4 samples with the default temperature (0.6)}
\PY{n}{generate\PYZus{}samples}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{I love languages.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
([[161, 724, 12202, 5112, 3, 1],
  [161, 12202, 5112, 3, 1],
  [161, 12202, 10, 5112, 3, 1],
  [161, 12202, 5112, 3, 1]],
 [-0.0002384185791015625,
  -0.0001735687255859375,
  -0.0001087188720703125,
  -0.0001735687255859375])
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{comparing-overlaps}{%
\subsubsection{4.2.2 Comparing overlaps}\label{comparing-overlaps}}

Let us now build our functions to compare a sample against another.
There are several metrics available as shown in the lectures and you can
try experimenting with any one of these. For this assignment, we will be
calculating scores for unigram overlaps. One of the more simple metrics
is the \href{https://en.wikipedia.org/wiki/Jaccard_index}{Jaccard
similarity} which gets the intersection over union of two sets. We've
already implemented it below for your perusal.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{jaccard\PYZus{}similarity}\PY{p}{(}\PY{n}{candidate}\PY{p}{,} \PY{n}{reference}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the Jaccard similarity between two token lists}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        candidate (list of int): tokenized version of the candidate translation}
\PY{l+s+sd}{        reference (list of int): tokenized version of the reference translation}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        float: overlap between the two token lists}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} convert the lists to a set to get the unique tokens}
    \PY{n}{can\PYZus{}unigram\PYZus{}set}\PY{p}{,} \PY{n}{ref\PYZus{}unigram\PYZus{}set} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{candidate}\PY{p}{)}\PY{p}{,} \PY{n+nb}{set}\PY{p}{(}\PY{n}{reference}\PY{p}{)}  
    
    \PY{c+c1}{\PYZsh{} get the set of tokens common to both candidate and reference}
    \PY{n}{joint\PYZus{}elems} \PY{o}{=} \PY{n}{can\PYZus{}unigram\PYZus{}set}\PY{o}{.}\PY{n}{intersection}\PY{p}{(}\PY{n}{ref\PYZus{}unigram\PYZus{}set}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} get the set of all tokens found in either candidate or reference}
    \PY{n}{all\PYZus{}elems} \PY{o}{=} \PY{n}{can\PYZus{}unigram\PYZus{}set}\PY{o}{.}\PY{n}{union}\PY{p}{(}\PY{n}{ref\PYZus{}unigram\PYZus{}set}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} divide the number of joint elements by the number of all elements}
    \PY{n}{overlap} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{joint\PYZus{}elems}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{all\PYZus{}elems}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{overlap}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} let\PYZsq{}s try using the function. remember the result here and compare with the next function below.}
\PY{n}{jaccard\PYZus{}similarity}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
0.75
\end{Verbatim}
\end{tcolorbox}
        
    One of the more commonly used metrics in machine translation is the
ROUGE score. For unigrams, this is called ROUGE-1 and as shown in class,
you can output the scores for both precision and recall when comparing
two samples. To get the final score, you will want to compute the
F1-score as given by:

\[score = 2* \frac{(precision * recall)}{(precision + recall)}\]

\#\#\# Exercise 08

\textbf{Instructions}: Implement the \texttt{rouge1\_similarity()}
function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C8}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}

\PY{c+c1}{\PYZsh{} for making a frequency table easily}
\PY{k+kn}{from} \PY{n+nn}{collections} \PY{k+kn}{import} \PY{n}{Counter}

\PY{k}{def} \PY{n+nf}{rouge1\PYZus{}similarity}\PY{p}{(}\PY{n}{system}\PY{p}{,} \PY{n}{reference}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the ROUGE\PYZhy{}1 score between two token lists}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        system (list of int): tokenized version of the system translation}
\PY{l+s+sd}{        reference (list of int): tokenized version of the reference translation}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        float: overlap between the two token lists}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}    
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} make a frequency table of the system tokens (hint: use the Counter class)}
    \PY{n}{sys\PYZus{}counter} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{n}{system}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} make a frequency table of the reference tokens (hint: use the Counter class)}
    \PY{n}{ref\PYZus{}counter} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{n}{reference}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} initialize overlap to 0}
    \PY{n}{overlap} \PY{o}{=} \PY{l+m+mi}{0}
    
    \PY{c+c1}{\PYZsh{} run a for loop over the sys\PYZus{}counter object (can be treated as a dictionary)}
    \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{sys\PYZus{}counter}\PY{p}{:}
        
        \PY{c+c1}{\PYZsh{} lookup the value of the token in the sys\PYZus{}counter dictionary (hint: use the get() method)}
        \PY{n}{token\PYZus{}count\PYZus{}sys} \PY{o}{=} \PY{n}{sys\PYZus{}counter}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{token}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} lookup the value of the token in the ref\PYZus{}counter dictionary (hint: use the get() method)}
        \PY{n}{token\PYZus{}count\PYZus{}ref} \PY{o}{=} \PY{n}{ref\PYZus{}counter}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{token}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} update the overlap by getting the smaller number between the two token counts above}
        \PY{n}{overlap} \PY{o}{+}\PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{token\PYZus{}count\PYZus{}sys}\PY{p}{,} \PY{n}{token\PYZus{}count\PYZus{}ref}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} get the precision (i.e. number of overlapping tokens / number of system tokens)}
    \PY{n}{precision} \PY{o}{=} \PY{n}{overlap} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{system}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} get the recall (i.e. number of overlapping tokens / number of reference tokens)}
    \PY{n}{recall} \PY{o}{=} \PY{n}{overlap} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{reference}\PY{p}{)}
    
    \PY{k}{if} \PY{n}{precision} \PY{o}{+} \PY{n}{recall} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} compute the f1\PYZhy{}score}
        \PY{n}{rouge1\PYZus{}score} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{p}{(}\PY{n}{precision} \PY{o}{*} \PY{n}{recall}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{precision} \PY{o}{+} \PY{n}{recall}\PY{p}{)}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{rouge1\PYZus{}score} \PY{o}{=} \PY{l+m+mi}{0} 
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{rouge1\PYZus{}score}
    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} notice that this produces a different value from the jaccard similarity earlier}
\PY{n}{rouge1\PYZus{}similarity}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
0.8571428571428571
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}rouge1\PYZus{}similarity}\PY{p}{(}\PY{n}{rouge1\PYZus{}similarity}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    \hypertarget{overall-score}{%
\subsubsection{4.2.3 Overall score}\label{overall-score}}

We will now build a function to generate the overall score for a
particular sample. As mentioned earlier, we need to compare each sample
with all other samples. For instance, if we generated 30 sentences, we
will need to compare sentence 1 to sentences 2 to 30. Then, we compare
sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we
get the average score of all comparisons to get the overall score for a
particular sample. To illustrate, these will be the steps to generate
the scores of a 4-sample list.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Get similarity score between sample 1 and sample 2
\item
  Get similarity score between sample 1 and sample 3
\item
  Get similarity score between sample 1 and sample 4
\item
  Get average score of the first 3 steps. This will be the overall score
  of sample 1.
\item
  Iterate and repeat until samples 1 to 4 have overall scores.
\end{enumerate}

We will be storing the results in a dictionary for easy lookups.

\#\#\# Exercise 09

\textbf{Instructions}: Implement the \texttt{average\_overlap()}
function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C9}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{average\PYZus{}overlap}\PY{p}{(}\PY{n}{similarity\PYZus{}fn}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{o}{*}\PY{n}{ignore\PYZus{}params}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the arithmetic mean of each candidate sentence in the samples}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        similarity\PYZus{}fn (function): similarity function used to compute the overlap}
\PY{l+s+sd}{        samples (list of lists): tokenized version of the translated sentences}
\PY{l+s+sd}{        *ignore\PYZus{}params: additional parameters will be ignored}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        dict: scores of each sample}
\PY{l+s+sd}{            key: index of the sample}
\PY{l+s+sd}{            value: score of the sample}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}  
    
    \PY{c+c1}{\PYZsh{} initialize dictionary}
    \PY{n}{scores} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
    
    \PY{c+c1}{\PYZsh{} run a for loop for each sample}
    \PY{k}{for} \PY{n}{index\PYZus{}candidate}\PY{p}{,} \PY{n}{candidate} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{samples}\PY{p}{)}\PY{p}{:}    
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
        
        \PY{c+c1}{\PYZsh{} initialize overlap to 0.0}
        \PY{n}{overlap} \PY{o}{=} \PY{l+m+mf}{0.0}
        
        \PY{c+c1}{\PYZsh{} run a for loop for each sample}
        \PY{k}{for} \PY{n}{index\PYZus{}sample}\PY{p}{,} \PY{n}{sample} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{samples}\PY{p}{)}\PY{p}{:} 

            \PY{c+c1}{\PYZsh{} skip if the candidate index is the same as the sample index}
            \PY{k}{if} \PY{n}{index\PYZus{}candidate} \PY{o}{==} \PY{n}{index\PYZus{}sample}\PY{p}{:}
                \PY{k}{continue}
                
            \PY{c+c1}{\PYZsh{} get the overlap between candidate and sample using the similarity function}
            \PY{n}{sample\PYZus{}overlap} \PY{o}{=} \PY{n}{similarity\PYZus{}fn}\PY{p}{(}\PY{n}{candidate}\PY{p}{,} \PY{n}{sample}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} add the sample overlap to the total overlap}
            \PY{n}{overlap} \PY{o}{+}\PY{o}{=} \PY{n}{sample\PYZus{}overlap}
            
        \PY{c+c1}{\PYZsh{} get the score for the candidate by computing the average}
        \PY{n}{score} \PY{o}{=} \PY{n}{overlap} \PY{o}{/} \PY{n}{index\PYZus{}sample}
        
        \PY{c+c1}{\PYZsh{} save the score in the dictionary. use index as the key.}
        \PY{n}{scores}\PY{p}{[}\PY{n}{index\PYZus{}candidate}\PY{p}{]} \PY{o}{=} \PY{n}{score}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{k}{return} \PY{n}{scores}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{average\PYZus{}overlap}\PY{p}{(}\PY{n}{jaccard\PYZus{}similarity}\PY{p}{,} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{0: 0.45, 1: 0.625, 2: 0.575\}
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}average\PYZus{}overlap}\PY{p}{(}\PY{n}{average\PYZus{}overlap}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{ All tests passed}
    \end{Verbatim}

    In practice, it is also common to see the weighted mean being used to
calculate the overall score instead of just the arithmetic mean. We have
implemented it below and you can use it in your experiements to see
which one will give better results.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{weighted\PYZus{}avg\PYZus{}overlap}\PY{p}{(}\PY{n}{similarity\PYZus{}fn}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{n}{log\PYZus{}probs}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the weighted mean of each candidate sentence in the samples}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        samples (list of lists): tokenized version of the translated sentences}
\PY{l+s+sd}{        log\PYZus{}probs (list of float): log probability of the translated sentences}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        dict: scores of each sample}
\PY{l+s+sd}{            key: index of the sample}
\PY{l+s+sd}{            value: score of the sample}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} initialize dictionary}
    \PY{n}{scores} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
    
    \PY{c+c1}{\PYZsh{} run a for loop for each sample}
    \PY{k}{for} \PY{n}{index\PYZus{}candidate}\PY{p}{,} \PY{n}{candidate} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{samples}\PY{p}{)}\PY{p}{:}    
        
        \PY{c+c1}{\PYZsh{} initialize overlap and weighted sum}
        \PY{n}{overlap}\PY{p}{,} \PY{n}{weight\PYZus{}sum} \PY{o}{=} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}
        
        \PY{c+c1}{\PYZsh{} run a for loop for each sample}
        \PY{k}{for} \PY{n}{index\PYZus{}sample}\PY{p}{,} \PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{logp}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{samples}\PY{p}{,} \PY{n}{log\PYZus{}probs}\PY{p}{)}\PY{p}{)}\PY{p}{:}

            \PY{c+c1}{\PYZsh{} skip if the candidate index is the same as the sample index            }
            \PY{k}{if} \PY{n}{index\PYZus{}candidate} \PY{o}{==} \PY{n}{index\PYZus{}sample}\PY{p}{:}
                \PY{k}{continue}
                
            \PY{c+c1}{\PYZsh{} convert log probability to linear scale}
            \PY{n}{sample\PYZus{}p} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{logp}\PY{p}{)}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} update the weighted sum}
            \PY{n}{weight\PYZus{}sum} \PY{o}{+}\PY{o}{=} \PY{n}{sample\PYZus{}p}

            \PY{c+c1}{\PYZsh{} get the unigram overlap between candidate and sample}
            \PY{n}{sample\PYZus{}overlap} \PY{o}{=} \PY{n}{similarity\PYZus{}fn}\PY{p}{(}\PY{n}{candidate}\PY{p}{,} \PY{n}{sample}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} update the overlap}
            \PY{n}{overlap} \PY{o}{+}\PY{o}{=} \PY{n}{sample\PYZus{}p} \PY{o}{*} \PY{n}{sample\PYZus{}overlap}
            
        \PY{c+c1}{\PYZsh{} get the score for the candidate}
        \PY{n}{score} \PY{o}{=} \PY{n}{overlap} \PY{o}{/} \PY{n}{weight\PYZus{}sum}
        
        \PY{c+c1}{\PYZsh{} save the score in the dictionary. use index as the key.}
        \PY{n}{scores}\PY{p}{[}\PY{n}{index\PYZus{}candidate}\PY{p}{]} \PY{o}{=} \PY{n}{score}
    
    \PY{k}{return} \PY{n}{scores}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{weighted\PYZus{}avg\PYZus{}overlap}\PY{p}{(}\PY{n}{jaccard\PYZus{}similarity}\PY{p}{,} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{0: 0.44255574831883415, 1: 0.631244796869735, 2: 0.5575581009406329\}
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{putting-it-all-together}{%
\subsubsection{4.2.4 Putting it all
together}\label{putting-it-all-together}}

We will now put everything together and develop the
\texttt{mbr\_decode()} function. Please use the helper functions you
just developed to complete this. You will want to generate samples, get
the score for each sample, get the highest score among all samples, then
detokenize this sample to get the translated sentence.

\#\#\# Exercise 10

\textbf{Instructions}: Implement the \texttt{mbr\_overlap()} function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C10}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION}
\PY{k}{def} \PY{n+nf}{mbr\PYZus{}decode}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{score\PYZus{}fn}\PY{p}{,} \PY{n}{similarity\PYZus{}fn}\PY{p}{,} \PY{n}{NMTAttn}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{temperature}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns the translated sentence using Minimum Bayes Risk decoding}

\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        sentence (str): sentence to translate.}
\PY{l+s+sd}{        n\PYZus{}samples (int): number of samples to generate}
\PY{l+s+sd}{        score\PYZus{}fn (function): function that generates the score for each sample}
\PY{l+s+sd}{        similarity\PYZus{}fn (function): function used to compute the overlap between a pair of samples}
\PY{l+s+sd}{        NMTAttn (tl.Serial): An LSTM sequence\PYZhy{}to\PYZhy{}sequence model with attention.}
\PY{l+s+sd}{        temperature (float): parameter for sampling ranging from 0.0 to 1.0.}
\PY{l+s+sd}{            0.0: same as argmax, always pick the most probable token}
\PY{l+s+sd}{            1.0: sampling from the distribution (can sometimes say random things)}
\PY{l+s+sd}{        vocab\PYZus{}file (str): filename of the vocabulary}
\PY{l+s+sd}{        vocab\PYZus{}dir (str): path to the vocabulary file}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        str: the translated sentence}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} generate samples}
    \PY{n}{samples}\PY{p}{,} \PY{n}{log\PYZus{}probs} \PY{o}{=} \PY{n}{generate\PYZus{}samples}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{NMTAttn}\PY{p}{,} \PY{n}{temperature}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} use the scoring function to get a dictionary of scores}
    \PY{c+c1}{\PYZsh{} pass in the relevant parameters as shown in the function definition of }
    \PY{c+c1}{\PYZsh{} the mean methods you developed earlier}
    \PY{n}{scores} \PY{o}{=} \PY{n}{score\PYZus{}fn}\PY{p}{(}\PY{n}{similarity\PYZus{}fn}\PY{p}{,} \PY{n}{samples}\PY{p}{,} \PY{n}{log\PYZus{}probs}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} find the key with the highest score}
    \PY{n}{max\PYZus{}index} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{scores}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{n}{scores}\PY{o}{.}\PY{n}{get}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} detokenize the token list associated with the max\PYZus{}index}
    \PY{n}{translated\PYZus{}sentence} \PY{o}{=} \PY{n}{detokenize}\PY{p}{(}\PY{n}{samples}\PY{p}{[}\PY{n}{max\PYZus{}index}\PY{p}{]}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{k}{return} \PY{p}{(}\PY{n}{translated\PYZus{}sentence}\PY{p}{,} \PY{n}{max\PYZus{}index}\PY{p}{,} \PY{n}{scores}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{TEMPERATURE} \PY{o}{=} \PY{l+m+mf}{1.0}

\PY{c+c1}{\PYZsh{} put a custom string here}
\PY{n}{your\PYZus{}sentence} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{She speaks English and German.}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{mbr\PYZus{}decode}\PY{p}{(}\PY{n}{your\PYZus{}sentence}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{weighted\PYZus{}avg\PYZus{}overlap}\PY{p}{,} \PY{n}{jaccard\PYZus{}similarity}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{TEMPERATURE}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
'Sie spricht Englisch und Deutsch.'
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{mbr\PYZus{}decode}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Congratulations!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{average\PYZus{}overlap}\PY{p}{,} \PY{n}{rouge1\PYZus{}similarity}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{TEMPERATURE}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        KeyboardInterrupt                         Traceback (most recent call last)

        <ipython-input-76-ca371b6b90ce> in <module>
    ----> 1 mbr\_decode('Congratulations!', 4, average\_overlap, rouge1\_similarity, model, TEMPERATURE, vocab\_file=VOCAB\_FILE, vocab\_dir=VOCAB\_DIR)[0]
    

        <ipython-input-73-0142de83dbcd> in mbr\_decode(sentence, n\_samples, score\_fn, similarity\_fn, NMTAttn, temperature, vocab\_file, vocab\_dir)
         22     \#\#\# START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) \#\#\#
         23     \# generate samples
    ---> 24     samples, log\_probs = generate\_samples(sentence, n\_samples, NMTAttn, temperature, vocab\_file, vocab\_dir)
         25 
         26     \# use the scoring function to get a dictionary of scores


        <ipython-input-52-1ed8b0750d75> in generate\_samples(sentence, n\_samples, NMTAttn, temperature, vocab\_file, vocab\_dir)
         24 
         25         \# get a sample using the sampling\_decode() function
    ---> 26         sample, logp, \_ = sampling\_decode(sentence, NMTAttn, temperature, vocab\_file=vocab\_file, vocab\_dir=vocab\_dir)
         27 
         28         \# append the token list to the samples list


        <ipython-input-46-7c7f35cf312b> in sampling\_decode(input\_sentence, NMTAttn, temperature, vocab\_file, vocab\_dir)
         38 
         39         \# update the current output token by getting the index of the next word (hint: use next\_symbol)
    ---> 40         cur\_output, log\_prob = next\_symbol(NMTAttn, input\_tokens, cur\_output\_tokens, temperature)
         41 
         42         \# append the current output token to the list of output tokens


        <ipython-input-44-e6614c9dc985> in next\_symbol(NMTAttn, input\_tokens, cur\_output\_tokens, temperature)
         35     \# get the model prediction. remember to use the `NMTAttn` argument defined above.
         36     \# hint: the model accepts a tuple as input (e.g. `my\_model((input1, input2))`)
    ---> 37     output, \_ = NMTAttn((input\_tokens, padded\_with\_batch))
         38 
         39     \# get log probabilities from the last token output


        /opt/conda/lib/python3.7/site-packages/trax/layers/base.py in \_\_call\_\_(self, x, weights, state, rng)
        171       self.state = state  \# Needed if the model wasn't fully initialized.
        172     state = self.state
    --> 173     outputs, new\_state = self.pure\_fn(x, weights, state, rng)
        174     self.state = new\_state
        175     self.weights = weights


        /opt/conda/lib/python3.7/site-packages/trax/layers/acceleration.py in pure\_fn(self, x, weights, state, rng, use\_cache)
         75       remainder = x.shape[0] \% self.\_n\_devices
         76     if remainder == 0:  \# If yes, run the accelerated sublayer.pure\_fn.
    ---> 77       return self.\_jit\_pure\_fn(x, weights, state, rng)
         78     \# If not, pad first.
         79     def pad(z):


        /opt/conda/lib/python3.7/site-packages/jax/api.py in f\_jitted(*args, **kwargs)
        168     flat\_fun, out\_tree = flatten\_fun(f, in\_tree)
        169     out = xla.xla\_call(flat\_fun, *args\_flat, device=device, backend=backend,
    --> 170                        name=flat\_fun.\_\_name\_\_, donated\_invars=donated\_invars)
        171     return tree\_unflatten(out\_tree(), out)
        172 


        /opt/conda/lib/python3.7/site-packages/jax/core.py in call\_bind(primitive, fun, *args, **params)
       1096   if top\_trace is None:
       1097     with new\_sublevel():
    -> 1098       outs = primitive.impl(fun, *args, **params)
       1099   else:
       1100     tracers = map(top\_trace.full\_raise, args)


        /opt/conda/lib/python3.7/site-packages/jax/interpreters/xla.py in \_xla\_call\_impl(fun, device, backend, name, donated\_invars, *args)
        538                                *unsafe\_map(arg\_spec, args))
        539   try:
    --> 540     return compiled\_fun(*args)
        541   except FloatingPointError:
        542     print("Invalid value encountered in the output of a jit function. "


        /opt/conda/lib/python3.7/site-packages/jax/interpreters/xla.py in \_execute\_compiled(compiled, handlers, *args)
        767 def \_execute\_compiled(compiled: XlaExecutable, handlers, *args):
        768   device, = compiled.local\_devices()
    --> 769   input\_bufs = [device\_put(x, device) for x in args if x is not token]
        770   out\_bufs = compiled.execute(input\_bufs)
        771   if FLAGS.jax\_debug\_nans: check\_nans(xla\_call\_p, out\_bufs)


        /opt/conda/lib/python3.7/site-packages/jax/interpreters/xla.py in <listcomp>(.0)
        767 def \_execute\_compiled(compiled: XlaExecutable, handlers, *args):
        768   device, = compiled.local\_devices()
    --> 769   input\_bufs = [device\_put(x, device) for x in args if x is not token]
        770   out\_bufs = compiled.execute(input\_bufs)
        771   if FLAGS.jax\_debug\_nans: check\_nans(xla\_call\_p, out\_bufs)


        /opt/conda/lib/python3.7/site-packages/jax/interpreters/xla.py in device\_put(x, device)
        118   x = canonicalize\_dtype(x)
        119   try:
    --> 120     return device\_put\_handlers[type(x)](x, device)
        121   except KeyError as err:
        122     raise TypeError(f"No device\_put handler for type: \{type(x)\}") from err


        /opt/conda/lib/python3.7/site-packages/jax/interpreters/xla.py in \_device\_put\_array(x, device)
        124 def \_device\_put\_array(x, device: Optional[Device]):
        125   backend = xb.get\_device\_backend(device)
    --> 126   return backend.buffer\_from\_pyval(x, device)
        127 
        128 def \_device\_put\_scalar(x, device):


        KeyboardInterrupt: 

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{mbr\PYZus{}decode}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{You have completed the assignment!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{average\PYZus{}overlap}\PY{p}{,} \PY{n}{rouge1\PYZus{}similarity}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{TEMPERATURE}\PY{p}{,} \PY{n}{vocab\PYZus{}file}\PY{o}{=}\PY{n}{VOCAB\PYZus{}FILE}\PY{p}{,} \PY{n}{vocab\PYZus{}dir}\PY{o}{=}\PY{n}{VOCAB\PYZus{}DIR}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \textbf{This unit test take a while to run. Please be patient}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} BEGIN UNIT TEST}
\PY{n}{w1\PYZus{}unittest}\PY{o}{.}\PY{n}{test\PYZus{}mbr\PYZus{}decode}\PY{p}{(}\PY{n}{mbr\PYZus{}decode}\PY{p}{,} \PY{n}{model}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END UNIT TEST}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{congratulations-next-week-youll-dive-deeper-into-attention-models-and-study-the-transformer-architecture.-you-will-build-another-network-but-without-the-recurrent-part.-it-will-show-that-attention-is-all-you-need-it-should-be-fun}{%
\paragraph{Congratulations! Next week, you'll dive deeper into attention
models and study the Transformer architecture. You will build another
network but without the recurrent part. It will show that attention is
all you need! It should be
fun!}\label{congratulations-next-week-youll-dive-deeper-into-attention-models-and-study-the-transformer-architecture.-you-will-build-another-network-but-without-the-recurrent-part.-it-will-show-that-attention-is-all-you-need-it-should-be-fun}}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
